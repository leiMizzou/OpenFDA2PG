"""
FDA Medical Device Database Quality Analyzer

This tool performs comprehensive analysis of the FDA medical device database to:
1. Identify database structure optimization opportunities
2. Detect data quality issues
3. Analyze data distributions and patterns
4. Provide recommendations for data preprocessing and analysis
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import psycopg2
from psycopg2.extras import RealDictCursor
import time
from tqdm.notebook import tqdm
from IPython.display import display, HTML, Markdown
import warnings
warnings.filterwarnings('ignore')

class DataQualityAnalyzer:
    """åˆ†æFDAåŒ»ç–—è®¾å¤‡æ•°æ®åº“çš„æ•°æ®è´¨é‡å’Œä¼˜åŒ–æœºä¼š"""
    
    def __init__(self, db_config):
        """åˆå§‹åŒ–æ•°æ®è´¨é‡åˆ†æå™¨"""
        self.db_config = db_config
        self.conn = None
        self.cur = None
        self.schema = "device"  # é»˜è®¤æ¨¡å¼
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.table_stats = {}
        self.index_analysis = {}
        self.null_analysis = {}
        self.duplicate_analysis = {}
        self.column_stats = {}
        self.relationship_analysis = {}
        self.query_recommendations = {}
        self.preprocessing_recommendations = {}
        
    def connect(self):
        """è¿æ¥åˆ°PostgreSQLæ•°æ®åº“"""
        dbname = self.db_config['dbname']
        
        try:
            self.conn = psycopg2.connect(**self.db_config)
            self.conn.autocommit = True  # è‡ªåŠ¨æäº¤
            self.cur = self.conn.cursor(cursor_factory=RealDictCursor)  # è¿”å›å­—å…¸ç»“æœ
            self.cur.execute(f"SET search_path TO {self.schema};")
            print(f"âœ… æˆåŠŸè¿æ¥åˆ°PostgreSQLæ•°æ®åº“ {dbname}")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.cur:
            self.cur.close()
        if self.conn:
            self.conn.close()
        print("ğŸ“Œ æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®è´¨é‡å’Œä¼˜åŒ–åˆ†æ"""
        display(HTML("<h1>FDAåŒ»ç–—è®¾å¤‡æ•°æ®åº“è´¨é‡åˆ†ææŠ¥å‘Š</h1>"))
        display(HTML(f"<p>ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>"))
        
        # 1. æ•°æ®åº“ç»“æ„åˆ†æ
        display(HTML("<h2>1. æ•°æ®åº“ç»“æ„åˆ†æ</h2>"))
        self.analyze_table_statistics()
        self.analyze_indexes()
        self.analyze_foreign_keys()
        
        # 2. æ•°æ®è´¨é‡åˆ†æ
        display(HTML("<h2>2. æ•°æ®è´¨é‡åˆ†æ</h2>"))
        self.analyze_null_values()
        self.analyze_duplicates()
        self.analyze_data_consistency()
        
        # 3. æ•°æ®å€¼åˆ†æ
        display(HTML("<h2>3. æ•°æ®å€¼åˆ†æ</h2>"))
        self.analyze_column_statistics()
        self.analyze_categorical_distributions()
        self.analyze_time_series_patterns()
        
        # 4. æ•°æ®å…³ç³»åˆ†æ
        display(HTML("<h2>4. æ•°æ®å…³ç³»åˆ†æ</h2>"))
        self.analyze_table_relationships()
        self.analyze_entity_connections()
        
        # 5. ä¼˜åŒ–å’Œå»ºè®®
        display(HTML("<h2>5. ä¼˜åŒ–å’Œå»ºè®®</h2>"))
        self.generate_optimization_recommendations()
        self.generate_query_recommendations()
        self.generate_preprocessing_recommendations()
        self.generate_analysis_recommendations()
        
        print("âœ… æ•°æ®åº“è´¨é‡åˆ†æå®Œæˆ")
    
    def analyze_table_statistics(self):
        """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"""
        display(HTML("<h3>è¡¨ç»Ÿè®¡ä¿¡æ¯</h3>"))
        
        try:
            # è·å–æ‰€æœ‰è¡¨å
            self.cur.execute(f"""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = '{self.schema}'
                AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """)
            
            tables = [row['table_name'] for row in self.cur.fetchall()]
            
            # åˆå§‹åŒ–ç»“æœ
            results = []
            
            for table in tqdm(tables, desc="åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"):
                # è¡¨è¡Œæ•°
                self.cur.execute(f"SELECT COUNT(*) as row_count FROM {self.schema}.{table}")
                row_count = self.cur.fetchone()['row_count']
                
                # è¡¨å¤§å°
                self.cur.execute(f"""
                    SELECT 
                        pg_size_pretty(pg_total_relation_size('{self.schema}.{table}')) as total_size,
                        pg_size_pretty(pg_relation_size('{self.schema}.{table}')) as table_size,
                        pg_size_pretty(pg_total_relation_size('{self.schema}.{table}') - 
                                        pg_relation_size('{self.schema}.{table}')) as index_size
                """)
                size_info = self.cur.fetchone()
                
                # åˆ—æ•°
                self.cur.execute(f"""
                    SELECT COUNT(*) as column_count
                    FROM information_schema.columns
                    WHERE table_schema = '{self.schema}'
                    AND table_name = '{table}'
                """)
                column_count = self.cur.fetchone()['column_count']
                
                # ä¸»é”®
                self.cur.execute(f"""
                    SELECT 
                        c.column_name
                    FROM 
                        information_schema.table_constraints tc
                    JOIN 
                        information_schema.constraint_column_usage AS ccu USING (constraint_schema, constraint_name)
                    JOIN 
                        information_schema.columns AS c 
                        ON c.table_schema = tc.constraint_schema
                        AND c.table_name = tc.table_name
                        AND c.column_name = ccu.column_name
                    WHERE 
                        constraint_type = 'PRIMARY KEY' 
                        AND tc.table_schema = '{self.schema}'
                        AND tc.table_name = '{table}'
                """)
                pk_columns = [row['column_name'] for row in self.cur.fetchall()]
                pk_info = ", ".join(pk_columns) if pk_columns else "æ— ä¸»é”®"
                
                # å­˜å‚¨ç»“æœ
                results.append({
                    'è¡¨å': table,
                    'è¡Œæ•°': row_count,
                    'æ€»å¤§å°': size_info['total_size'],
                    'è¡¨å¤§å°': size_info['table_size'],
                    'ç´¢å¼•å¤§å°': size_info['index_size'],
                    'åˆ—æ•°': column_count,
                    'ä¸»é”®': pk_info
                })
                
                # å­˜å‚¨è¯¦ç»†ä¿¡æ¯ä¾›åç»­åˆ†æä½¿ç”¨
                self.table_stats[table] = {
                    'row_count': row_count,
                    'total_size': size_info['total_size'],
                    'table_size': size_info['table_size'],
                    'index_size': size_info['index_size'],
                    'column_count': column_count,
                    'primary_key': pk_columns
                }
            
            # æ˜¾ç¤ºç»“æœ
            result_df = pd.DataFrame(results)
            display(result_df.sort_values(by='è¡Œæ•°', ascending=False))
            
            # æä¾›ä¸»è¦è¡¨çš„æ‘˜è¦
            main_tables = result_df[result_df['è¡Œæ•°'] > 1000].sort_values(by='è¡Œæ•°', ascending=False)
            
            display(HTML("<h4>ä¸»è¦è¡¨æ‘˜è¦</h4>"))
            display(Markdown(f"""
            æ•°æ®åº“ä¸­å…±æœ‰ **{len(tables)}** ä¸ªè¡¨ï¼Œæ€»è¡Œæ•°è¶…è¿‡ **{result_df['è¡Œæ•°'].sum():,}**ã€‚

            æœ€å¤§çš„è¡¨æ˜¯:
            - **{main_tables.iloc[0]['è¡¨å']}**: {main_tables.iloc[0]['è¡Œæ•°']:,} è¡Œ ({main_tables.iloc[0]['æ€»å¤§å°']})
            - **{main_tables.iloc[1]['è¡¨å']}**: {main_tables.iloc[1]['è¡Œæ•°']:,} è¡Œ ({main_tables.iloc[1]['æ€»å¤§å°']})
            - **{main_tables.iloc[2]['è¡¨å']}**: {main_tables.iloc[2]['è¡Œæ•°']:,} è¡Œ ({main_tables.iloc[2]['æ€»å¤§å°']})

            * ä¸»é”®ç¼ºå¤±çš„è¡¨: **{sum(1 for row in results if row['ä¸»é”®'] == 'æ— ä¸»é”®')}** ä¸ª
            * æœ€å¤§è¡¨ç´¢å¼•å æ¯”: **{main_tables.iloc[0]['ç´¢å¼•å¤§å°']}** / **{main_tables.iloc[0]['æ€»å¤§å°']}**
            """))
            
            # å¯è§†åŒ–å‰10å¤§è¡¨
            top10_tables = result_df.nlargest(10, 'è¡Œæ•°')
            plt.figure(figsize=(12, 6))
            sns.barplot(x='è¡¨å', y='è¡Œæ•°', data=top10_tables)
            plt.xticks(rotation=45, ha='right')
            plt.title('æ•°æ®åº“ä¸­å‰10å¤§è¡¨ï¼ˆæŒ‰è¡Œæ•°ï¼‰')
            plt.tight_layout()
            plt.show()
            
            # å­˜å‚¨å¼‚å¸¸å‘ç°
            findings = []
            
            # æ£€æŸ¥æ— ä¸»é”®çš„è¡¨
            no_pk_tables = [row['è¡¨å'] for row in results if row['ä¸»é”®'] == 'æ— ä¸»é”®']
            if no_pk_tables:
                findings.append(f"å‘ç° {len(no_pk_tables)} ä¸ªè¡¨æ²¡æœ‰ä¸»é”®: {', '.join(no_pk_tables)}")
            
            # æ£€æŸ¥è¶…å¤§è¡¨
            very_large_tables = [row['è¡¨å'] for row in results if row['è¡Œæ•°'] > 10000000]
            if very_large_tables:
                findings.append(f"å‘ç° {len(very_large_tables)} ä¸ªéå¸¸å¤§çš„è¡¨ (>1000ä¸‡è¡Œ): {', '.join(very_large_tables)}")
            
            # æ£€æŸ¥æå°è¡¨ï¼ˆå¯èƒ½æ˜¯æ²¡ç”¨çš„è¡¨ï¼‰
            very_small_tables = [row['è¡¨å'] for row in results if row['è¡Œæ•°'] < 10 and row['è¡¨å'] not in ['dataset_metadata']]
            if very_small_tables:
                findings.append(f"å‘ç° {len(very_small_tables)} ä¸ªæå°çš„è¡¨ (<10è¡Œ): {', '.join(very_small_tables)}")
            
            if findings:
                display(HTML("<h4>ç»“æ„é—®é¢˜å‘ç°</h4>"))
                for finding in findings:
                    display(Markdown(f"- {finding}"))
            
        except Exception as e:
            print(f"âŒ åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_indexes(self):
        """åˆ†æç´¢å¼•æƒ…å†µ"""
        display(HTML("<h3>ç´¢å¼•åˆ†æ</h3>"))
        
        try:
            # è·å–æ‰€æœ‰ç´¢å¼•ä¿¡æ¯
            self.cur.execute(f"""
                SELECT
                    t.relname AS table_name,
                    i.relname AS index_name,
                    a.attname AS column_name,
                    ix.indisunique AS is_unique,
                    ix.indisprimary AS is_primary,
                    pg_size_pretty(pg_relation_size(i.oid)) AS index_size
                FROM
                    pg_class t,
                    pg_class i,
                    pg_index ix,
                    pg_attribute a
                WHERE
                    t.oid = ix.indrelid
                    AND i.oid = ix.indexrelid
                    AND a.attrelid = t.oid
                    AND a.attnum = ANY(ix.indkey)
                    AND t.relkind = 'r'
                    AND t.relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = '{self.schema}')
                ORDER BY
                    t.relname,
                    i.relname
            """)
            
            index_data = self.cur.fetchall()
            
            # å°†ç´¢å¼•æ•°æ®è½¬æ¢ä¸ºæ¯ä¸ªè¡¨åŠå…¶ç´¢å¼•çš„ç»“æ„
            table_indexes = {}
            for row in index_data:
                table_name = row['table_name']
                index_name = row['index_name']
                
                if table_name not in table_indexes:
                    table_indexes[table_name] = {}
                
                if index_name not in table_indexes[table_name]:
                    table_indexes[table_name][index_name] = {
                        'columns': [],
                        'is_unique': row['is_unique'],
                        'is_primary': row['is_primary'],
                        'size': row['index_size']
                    }
                
                table_indexes[table_name][index_name]['columns'].append(row['column_name'])
            
            # åˆ†ææ¯ä¸ªè¡¨çš„ç´¢å¼•æƒ…å†µ
            index_analysis = []
            
            for table_name, indexes in table_indexes.items():
                # è·³è¿‡å°è¡¨
                if table_name in self.table_stats and self.table_stats[table_name]['row_count'] < 100:
                    continue
                
                # åˆå¹¶åˆ—å
                for index_name, index_info in indexes.items():
                    index_info['columns'] = ", ".join(index_info['columns'])
                
                # è®¡ç®—ç´¢å¼•å’Œè¡¨çš„æ¯”ç‡
                row_count = self.table_stats.get(table_name, {}).get('row_count', 0)
                total_indexes = len(indexes)
                
                has_primary = any(index_info['is_primary'] for index_info in indexes.values())
                unique_indexes = sum(1 for index_info in indexes.values() if index_info['is_unique'])
                
                # æ·»åŠ åˆ°åˆ†æç»“æœ
                index_analysis.append({
                    'è¡¨å': table_name,
                    'è¡Œæ•°': row_count,
                    'ç´¢å¼•æ•°': total_indexes,
                    'æœ‰ä¸»é”®ç´¢å¼•': 'æ˜¯' if has_primary else 'å¦',
                    'å”¯ä¸€ç´¢å¼•æ•°': unique_indexes,
                    'ç´¢å¼•åˆ—è¡¨': ", ".join(indexes.keys())
                })
                
                # å­˜å‚¨ç´¢å¼•åˆ†æç»“æœ
                self.index_analysis[table_name] = {
                    'indexes': list(indexes.keys()),
                    'total_indexes': total_indexes,
                    'has_primary': has_primary,
                    'unique_indexes': unique_indexes,
                    'index_details': indexes
                }
            
            # æ˜¾ç¤ºç»“æœ
            index_df = pd.DataFrame(index_analysis)
            display(index_df.sort_values(by=['è¡Œæ•°', 'ç´¢å¼•æ•°'], ascending=[False, False]))
            
            # å¯»æ‰¾ç´¢å¼•ä¼˜åŒ–æœºä¼š
            display(HTML("<h4>ç´¢å¼•ä¼˜åŒ–æœºä¼š</h4>"))
            
            # å¤§è¡¨ä½†ç´¢å¼•å°‘
            large_tables_few_indexes = index_df[(index_df['è¡Œæ•°'] > 100000) & (index_df['ç´¢å¼•æ•°'] < 3)]
            if not large_tables_few_indexes.empty:
                display(Markdown(f"**å¤§è¡¨ç´¢å¼•ä¸è¶³**: ä»¥ä¸‹è¡¨åŒ…å«è¶…è¿‡10ä¸‡è¡Œæ•°æ®ä½†ç´¢å¼•å°‘äº3ä¸ª:"))
                display(large_tables_few_indexes[['è¡¨å', 'è¡Œæ•°', 'ç´¢å¼•æ•°', 'æœ‰ä¸»é”®ç´¢å¼•']])
            
            # æ²¡æœ‰ä¸»é”®çš„è¡¨
            tables_without_pk = index_df[index_df['æœ‰ä¸»é”®ç´¢å¼•'] == 'å¦']
            if not tables_without_pk.empty:
                display(Markdown(f"**ç¼ºå°‘ä¸»é”®ç´¢å¼•**: ä»¥ä¸‹è¡¨æ²¡æœ‰ä¸»é”®ç´¢å¼•:"))
                display(tables_without_pk[['è¡¨å', 'è¡Œæ•°']])
            
            # å¯èƒ½å­˜åœ¨ç´¢å¼•å†—ä½™çš„è¡¨
            tables_many_indexes = index_df[(index_df['ç´¢å¼•æ•°'] > 5)]
            if not tables_many_indexes.empty:
                display(Markdown(f"**å¯èƒ½å­˜åœ¨ç´¢å¼•å†—ä½™**: ä»¥ä¸‹è¡¨ç´¢å¼•æ•°é‡è¾ƒå¤šï¼Œå¯èƒ½å­˜åœ¨å†—ä½™:"))
                display(tables_many_indexes[['è¡¨å', 'è¡Œæ•°', 'ç´¢å¼•æ•°', 'ç´¢å¼•åˆ—è¡¨']])
        
        except Exception as e:
            print(f"âŒ åˆ†æç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_foreign_keys(self):
        """åˆ†æå¤–é”®å…³ç³»"""
        display(HTML("<h3>å¤–é”®å…³ç³»åˆ†æ</h3>"))
        
        try:
            # è·å–æ‰€æœ‰å¤–é”®å…³ç³»
            self.cur.execute(f"""
                SELECT
                    tc.table_name AS table_name,
                    kcu.column_name AS column_name,
                    ccu.table_name AS foreign_table_name,
                    ccu.column_name AS foreign_column_name
                FROM
                    information_schema.table_constraints AS tc
                JOIN
                    information_schema.key_column_usage AS kcu
                    ON tc.constraint_name = kcu.constraint_name
                JOIN
                    information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                WHERE
                    tc.constraint_type = 'FOREIGN KEY'
                    AND tc.table_schema = '{self.schema}'
                ORDER BY
                    tc.table_name,
                    kcu.column_name
            """)
            
            fk_data = self.cur.fetchall()
            
            if not fk_data:
                display(Markdown("æ•°æ®åº“ä¸­æ²¡æœ‰å®šä¹‰å¤–é”®å…³ç³»ã€‚è¿™å¯èƒ½å¯¼è‡´æ•°æ®å®Œæ•´æ€§é—®é¢˜ï¼Œå»ºè®®è€ƒè™‘æ·»åŠ é€‚å½“çš„å¤–é”®çº¦æŸã€‚"))
                return
            
            # æ„å»ºå¤–é”®å…³ç³»åˆ—è¡¨
            fk_relations = []
            for row in fk_data:
                fk_relations.append({
                    'è¡¨å': row['table_name'],
                    'åˆ—å': row['column_name'],
                    'å¼•ç”¨è¡¨': row['foreign_table_name'],
                    'å¼•ç”¨åˆ—': row['foreign_column_name']
                })
            
            fk_df = pd.DataFrame(fk_relations)
            display(fk_df)
            
            # è®¡ç®—æ¯ä¸ªè¡¨çš„å¤–é”®æ•°
            table_fk_counts = fk_df['è¡¨å'].value_counts().to_dict()
            
            # æ£€æŸ¥å¤–é”®ç´¢å¼•æƒ…å†µ
            fk_index_status = []
            
            for row in fk_relations:
                table_name = row['è¡¨å']
                column_name = row['åˆ—å']
                
                # æ£€æŸ¥å¤–é”®åˆ—æ˜¯å¦æœ‰ç´¢å¼•
                self.cur.execute(f"""
                    SELECT
                        i.relname AS index_name
                    FROM
                        pg_class t,
                        pg_class i,
                        pg_index ix,
                        pg_attribute a
                    WHERE
                        t.oid = ix.indrelid
                        AND i.oid = ix.indexrelid
                        AND a.attrelid = t.oid
                        AND a.attnum = ANY(ix.indkey)
                        AND t.relkind = 'r'
                        AND t.relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = '{self.schema}')
                        AND t.relname = '{table_name}'
                        AND a.attname = '{column_name}'
                """)
                
                has_index = len(self.cur.fetchall()) > 0
                
                fk_index_status.append({
                    'è¡¨å': table_name,
                    'å¤–é”®åˆ—': column_name,
                    'å¼•ç”¨è¡¨': row['å¼•ç”¨è¡¨'],
                    'å¼•ç”¨åˆ—': row['å¼•ç”¨åˆ—'],
                    'æœ‰ç´¢å¼•': 'æ˜¯' if has_index else 'å¦'
                })
            
            # æ˜¾ç¤ºå¤–é”®ç´¢å¼•çŠ¶æ€
            display(HTML("<h4>å¤–é”®ç´¢å¼•çŠ¶æ€</h4>"))
            fk_index_df = pd.DataFrame(fk_index_status)
            display(fk_index_df)
            
            # æ‰¾å‡ºæ²¡æœ‰ç´¢å¼•çš„å¤–é”®
            fk_without_index = fk_index_df[fk_index_df['æœ‰ç´¢å¼•'] == 'å¦']
            
            if not fk_without_index.empty:
                display(HTML("<h4>ä¼˜åŒ–å»ºè®®ï¼šå¤–é”®ç´¢å¼•</h4>"))
                display(Markdown("ä»¥ä¸‹å¤–é”®æ²¡æœ‰ç´¢å¼•ï¼Œä¸ºå®ƒä»¬æ·»åŠ ç´¢å¼•å¯ä»¥æ”¹å–„è¿æ¥æŸ¥è¯¢æ€§èƒ½:"))
                display(fk_without_index)
                
                # ç”Ÿæˆåˆ›å»ºç´¢å¼•çš„SQL
                display(Markdown("**å»ºè®®çš„ç´¢å¼•åˆ›å»ºè¯­å¥:**"))
                
                for _, row in fk_without_index.iterrows():
                    table = row['è¡¨å']
                    column = row['å¤–é”®åˆ—']
                    index_name = f"idx_{table}_{column}"
                    
                    sql = f"CREATE INDEX {index_name} ON {self.schema}.{table} ({column});"
                    display(Markdown(f"```sql\n{sql}\n```"))
            
            # æ£€æŸ¥å¤–é”®æ•°æ®å®Œæ•´æ€§
            display(HTML("<h4>å¤–é”®æ•°æ®å®Œæ•´æ€§æ£€æŸ¥</h4>"))
            
            integrity_issues = []
            for _, row in fk_df.iterrows():
                table = row['è¡¨å']
                column = row['åˆ—å']
                ref_table = row['å¼•ç”¨è¡¨']
                ref_column = row['å¼•ç”¨åˆ—']
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤–é”®å€¼åœ¨å¼•ç”¨è¡¨ä¸­ä¸å­˜åœ¨
                self.cur.execute(f"""
                    SELECT COUNT(*) as invalid_count
                    FROM {self.schema}.{table} t
                    LEFT JOIN {self.schema}.{ref_table} r ON t.{column} = r.{ref_column}
                    WHERE t.{column} IS NOT NULL AND r.{ref_column} IS NULL
                """)
                
                result = self.cur.fetchone()
                invalid_count = result['invalid_count'] if result else 0
                
                if invalid_count > 0:
                    integrity_issues.append({
                        'è¡¨å': table,
                        'å¤–é”®åˆ—': column,
                        'å¼•ç”¨è¡¨': ref_table,
                        'å¼•ç”¨åˆ—': ref_column,
                        'æ— æ•ˆå¼•ç”¨æ•°': invalid_count
                    })
            
            if integrity_issues:
                display(Markdown("**æ£€æµ‹åˆ°å¤–é”®æ•°æ®å®Œæ•´æ€§é—®é¢˜:**"))
                display(pd.DataFrame(integrity_issues))
                
                # æä¾›ä¿®å¤å»ºè®®
                display(Markdown("**å»ºè®®:**"))
                display(Markdown("- æ£€æŸ¥æ•°æ®å¯¼å…¥è¿‡ç¨‹ï¼Œç¡®ä¿å¤–é”®å®Œæ•´æ€§\n- è€ƒè™‘æ·»åŠ å¤–é”®çº¦æŸä»¥é˜²æ­¢å°†æ¥å‡ºç°æ— æ•ˆå¼•ç”¨\n- å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æŸ¥è¯¢è¯†åˆ«å…·ä½“çš„æ— æ•ˆå¼•ç”¨è®°å½•:"))
                
                for issue in integrity_issues:
                    table = issue['è¡¨å']
                    column = issue['å¤–é”®åˆ—']
                    ref_table = issue['å¼•ç”¨è¡¨']
                    ref_column = issue['å¼•ç”¨åˆ—']
                    
                    sql = f"""
                    SELECT t.*
                    FROM {self.schema}.{table} t
                    LEFT JOIN {self.schema}.{ref_table} r ON t.{column} = r.{ref_column}
                    WHERE t.{column} IS NOT NULL AND r.{ref_column} IS NULL
                    LIMIT 10;
                    """
                    
                    display(Markdown(f"```sql\n{sql}\n```"))
            else:
                display(Markdown("âœ… æœªå‘ç°å¤–é”®æ•°æ®å®Œæ•´æ€§é—®é¢˜ã€‚"))
        
        except Exception as e:
            print(f"âŒ åˆ†æå¤–é”®å…³ç³»æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_null_values(self):
        """åˆ†æç©ºå€¼æƒ…å†µ"""
        display(HTML("<h3>ç©ºå€¼åˆ†æ</h3>"))
        
        try:
            # è·å–ä¸»è¦è¡¨ï¼ˆè·³è¿‡ç©ºè¡¨å’Œéå¸¸å°çš„è¡¨ï¼‰
            main_tables = [table for table, stats in self.table_stats.items() 
                          if stats['row_count'] > 100]
            
            null_analysis_results = []
            
            for table in tqdm(main_tables, desc="åˆ†æç©ºå€¼"):
                # è·å–è¡¨çš„åˆ—
                self.cur.execute(f"""
                    SELECT column_name, data_type
                    FROM information_schema.columns
                    WHERE table_schema = '{self.schema}'
                    AND table_name = '{table}'
                    ORDER BY ordinal_position
                """)
                
                columns = self.cur.fetchall()
                
                for column in columns:
                    column_name = column['column_name']
                    data_type = column['data_type']
                    
                    # è®¡ç®—ç©ºå€¼ç™¾åˆ†æ¯”
                    self.cur.execute(f"""
                        SELECT 
                            COUNT(*) as total_count,
                            SUM(CASE WHEN {column_name} IS NULL THEN 1 ELSE 0 END) as null_count
                        FROM {self.schema}.{table}
                    """)
                    
                    result = self.cur.fetchone()
                    total_count = result['total_count']
                    null_count = result['null_count']
                    
                    if total_count > 0:
                        null_percentage = (null_count / total_count) * 100
                    else:
                        null_percentage = 0
                    
                    null_analysis_results.append({
                        'è¡¨å': table,
                        'åˆ—å': column_name,
                        'æ•°æ®ç±»å‹': data_type,
                        'æ€»è®°å½•æ•°': total_count,
                        'ç©ºå€¼æ•°': null_count,
                        'ç©ºå€¼ç™¾åˆ†æ¯”': round(null_percentage, 2)
                    })
                    
                    # å­˜å‚¨ç©ºå€¼åˆ†æç»“æœ
                    if table not in self.null_analysis:
                        self.null_analysis[table] = {}
                    
                    self.null_analysis[table][column_name] = {
                        'data_type': data_type,
                        'null_count': null_count,
                        'null_percentage': null_percentage
                    }
            
            null_df = pd.DataFrame(null_analysis_results)
            
            # ç©ºå€¼ç™¾åˆ†æ¯”è¾ƒé«˜çš„åˆ—
            high_null_df = null_df[null_df['ç©ºå€¼ç™¾åˆ†æ¯”'] > 50].sort_values(by='ç©ºå€¼ç™¾åˆ†æ¯”', ascending=False)
            
            if not high_null_df.empty:
                display(HTML("<h4>ç©ºå€¼æ¯”ä¾‹é«˜çš„åˆ—</h4>"))
                display(high_null_df.head(20))
                
                # æŒ‰è¡¨åˆ†ç»„æ˜¾ç¤ºç©ºå€¼æ¯”ä¾‹é«˜çš„åˆ—
                display(HTML("<h4>æ¯ä¸ªè¡¨ç©ºå€¼æ¯”ä¾‹é«˜çš„åˆ—</h4>"))
                
                for table in main_tables:
                    table_high_nulls = null_df[(null_df['è¡¨å'] == table) & (null_df['ç©ºå€¼ç™¾åˆ†æ¯”'] > 30)]
                    
                    if not table_high_nulls.empty:
                        display(Markdown(f"**è¡¨ {table}:**"))
                        display(table_high_nulls[['åˆ—å', 'æ•°æ®ç±»å‹', 'ç©ºå€¼ç™¾åˆ†æ¯”']].sort_values(by='ç©ºå€¼ç™¾åˆ†æ¯”', ascending=False))
                
                # å¯è§†åŒ–æ’åå‰10çš„é«˜ç©ºå€¼åˆ—
                plt.figure(figsize=(12, 6))
                top_nulls = high_null_df.head(15)
                sns.barplot(x='ç©ºå€¼ç™¾åˆ†æ¯”', y='åˆ—å', hue='è¡¨å', data=top_nulls)
                plt.title('ç©ºå€¼æ¯”ä¾‹æœ€é«˜çš„15ä¸ªåˆ—')
                plt.tight_layout()
                plt.show()
            
            # æä¾›åˆ†æå’Œå»ºè®®
            display(HTML("<h4>ç©ºå€¼åˆ†æä¸å»ºè®®</h4>"))
            
            # æ‰¾å‡ºå…·æœ‰è¾ƒå¤šç©ºå€¼çš„ä¸»è¦è¡¨
            high_null_tables = {}
            for table in main_tables:
                table_nulls = null_df[null_df['è¡¨å'] == table]
                avg_null_pct = table_nulls['ç©ºå€¼ç™¾åˆ†æ¯”'].mean()
                high_null_tables[table] = avg_null_pct
            
            # æŒ‰ç©ºå€¼ç™¾åˆ†æ¯”æ’åº
            high_null_tables = {k: v for k, v in sorted(high_null_tables.items(), key=lambda item: item[1], reverse=True)}
            
            top_high_null_tables = list(high_null_tables.items())[:5]
            
            if top_high_null_tables:
                display(Markdown("**ç©ºå€¼æ¯”ä¾‹è¾ƒé«˜çš„è¡¨:**"))
                for table, null_pct in top_high_null_tables:
                    display(Markdown(f"- **{table}**: å¹³å‡ç©ºå€¼æ¯”ä¾‹ {null_pct:.2f}%"))
            
            # æä¾›å¤„ç†ç©ºå€¼çš„å»ºè®®
            display(Markdown("""
            **å¤„ç†ç©ºå€¼çš„å»ºè®®:**
            
            1. **å¯¹äºç©ºå€¼æ¯”ä¾‹æé«˜çš„åˆ— (>90%):**
               - è€ƒè™‘æ˜¯å¦çœŸæ­£éœ€è¦è¿™äº›å­—æ®µ
               - æ£€æŸ¥æ•°æ®æ”¶é›†æµç¨‹ï¼Œç¡®è®¤ä¸ºä»€ä¹ˆè¿™äº›å­—æ®µå¤§å¤šä¸ºç©º
               
            2. **å¯¹äºç©ºå€¼æ¯”ä¾‹ä¸­ç­‰çš„åˆ— (30-90%):**
               - ä¸ºåˆ†æç›®çš„å¡«å……ç©ºå€¼ (å‡å€¼ã€ä¸­ä½æ•°ã€ä¼—æ•°ç­‰)
               - è€ƒè™‘æ·»åŠ "ç¼ºå¤±"æ ‡å¿—åˆ—ä»¥ä¿ç•™ç©ºå€¼ä¿¡æ¯
               - åˆ†æç©ºå€¼çš„åˆ†å¸ƒæ¨¡å¼ï¼Œç¡®å®šæ˜¯å¦ä¸ç‰¹å®šæ¡ä»¶ç›¸å…³
               
            3. **å¯¹äºå…³é”®å­—æ®µä¸­çš„ç©ºå€¼:**
               - æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œè´¨é‡æ§åˆ¶æµç¨‹
               - è€ƒè™‘ä½¿ç”¨ä¸šåŠ¡è§„åˆ™æˆ–å¤–éƒ¨æ•°æ®æºå¡«å……
               - ç¡®å®šè¿™äº›ç©ºå€¼æ˜¯å¦è¡¨ç¤ºçœŸå®çš„"æœªçŸ¥"æˆ–æ˜¯æ•°æ®é—®é¢˜
            """))
        
        except Exception as e:
            print(f"âŒ åˆ†æç©ºå€¼æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_duplicates(self):
        """åˆ†æé‡å¤è®°å½•"""
        display(HTML("<h3>é‡å¤è®°å½•åˆ†æ</h3>"))
        
        try:
            # é€‰æ‹©ä¸»è¦è¡¨è¿›è¡Œåˆ†æ
            main_tables = {
                'adverse_events': 'report_number',
                'device_recalls': 'recall_number',
                'enforcement_actions': 'recall_number',
                'udi_records': 'public_device_record_key',
                'event_devices': ['event_id', 'device_sequence_number'],
                'product_codes': 'product_code'
            }
            
            duplicate_results = []
            
            for table, key_columns in main_tables.items():
                # ç¡®ä¿è¡¨å­˜åœ¨
                if table not in self.table_stats:
                    continue
                
                # å¦‚æœåªæœ‰ä¸€ä¸ªé”®åˆ—
                if isinstance(key_columns, str):
                    self.cur.execute(f"""
                        SELECT 
                            '{table}' as table_name, 
                            '{key_columns}' as key_column,
                            COUNT(*) as total_records,
                            COUNT(DISTINCT {key_columns}) as unique_keys,
                            COUNT(*) - COUNT(DISTINCT {key_columns}) as duplicate_count,
                            CASE 
                                WHEN COUNT(*) > 0 THEN 
                                    ROUND(((COUNT(*) - COUNT(DISTINCT {key_columns}))::float / COUNT(*) * 100), 2)
                                ELSE 0 
                            END as duplicate_percentage
                        FROM {self.schema}.{table}
                        WHERE {key_columns} IS NOT NULL
                    """)
                else:
                    # å¤šä¸ªé”®åˆ—çš„æƒ…å†µ
                    key_columns_str = ', '.join(key_columns)
                    self.cur.execute(f"""
                        WITH KeyCounts AS (
                            SELECT 
                                {key_columns_str},
                                COUNT(*) as key_count
                            FROM {self.schema}.{table}
                            GROUP BY {key_columns_str}
                            HAVING COUNT(*) > 1
                        )
                        SELECT 
                            '{table}' as table_name, 
                            '{key_columns_str}' as key_column,
                            (SELECT COUNT(*) FROM {self.schema}.{table}) as total_records,
                            (SELECT COUNT(*) FROM (SELECT DISTINCT {key_columns_str} FROM {self.schema}.{table}) t) as unique_keys,
                            (SELECT SUM(key_count) - COUNT(*) FROM KeyCounts) as duplicate_count,
                            CASE 
                                WHEN (SELECT COUNT(*) FROM {self.schema}.{table}) > 0 THEN 
                                    ROUND(((SELECT COALESCE(SUM(key_count) - COUNT(*), 0) FROM KeyCounts)::float / 
                                           (SELECT COUNT(*) FROM {self.schema}.{table}) * 100), 2)
                                ELSE 0 
                            END as duplicate_percentage
                    """)
                
                result = self.cur.fetchone()
                
                if result and result['total_records'] > 0:
                    duplicate_results.append({
                        'è¡¨å': result['table_name'],
                        'é”®åˆ—': result['key_column'],
                        'æ€»è®°å½•æ•°': result['total_records'],
                        'å”¯ä¸€é”®æ•°': result['unique_keys'],
                        'é‡å¤è®°å½•æ•°': result['duplicate_count'] or 0,
                        'é‡å¤ç™¾åˆ†æ¯”': result['duplicate_percentage'] or 0
                    })
                    
                    # å¦‚æœå­˜åœ¨é‡å¤ï¼Œè·å–ä¸€äº›ç¤ºä¾‹
                    if result['duplicate_count'] and result['duplicate_count'] > 0:
                        if isinstance(key_columns, str):
                            self.cur.execute(f"""
                                WITH DuplicateKeys AS (
                                    SELECT {key_columns}
                                    FROM {self.schema}.{table}
                                    GROUP BY {key_columns}
                                    HAVING COUNT(*) > 1
                                    LIMIT 5
                                )
                                SELECT {key_columns}, COUNT(*) as count
                                FROM {self.schema}.{table}
                                WHERE {key_columns} IN (SELECT {key_columns} FROM DuplicateKeys)
                                GROUP BY {key_columns}
                                ORDER BY count DESC
                            """)
                        else:
                            key_columns_str = ', '.join(key_columns)
                            self.cur.execute(f"""
                                WITH DuplicateKeys AS (
                                    SELECT {key_columns_str}
                                    FROM {self.schema}.{table}
                                    GROUP BY {key_columns_str}
                                    HAVING COUNT(*) > 1
                                    LIMIT 5
                                )
                                SELECT {key_columns_str}, COUNT(*) as count
                                FROM {self.schema}.{table}
                                WHERE ({key_columns_str}) IN (SELECT {key_columns_str} FROM DuplicateKeys)
                                GROUP BY {key_columns_str}
                                ORDER BY count DESC
                            """)
                        
                        duplicate_examples = self.cur.fetchall()
                        
                        # å­˜å‚¨é‡å¤ç¤ºä¾‹
                        if table not in self.duplicate_analysis:
                            self.duplicate_analysis[table] = {
                                'key_columns': key_columns,
                                'duplicate_count': result['duplicate_count'],
                                'duplicate_percentage': result['duplicate_percentage'],
                                'examples': duplicate_examples
                            }
            
            # æ˜¾ç¤ºç»“æœ
            duplicate_df = pd.DataFrame(duplicate_results)
            display(duplicate_df.sort_values(by='é‡å¤ç™¾åˆ†æ¯”', ascending=False))
            
            # å¦‚æœå­˜åœ¨é‡å¤è®°å½•ï¼Œæ˜¾ç¤ºç¤ºä¾‹
            tables_with_duplicates = [row['è¡¨å'] for _, row in duplicate_df.iterrows() 
                                     if row['é‡å¤è®°å½•æ•°'] > 0]
            
            if tables_with_duplicates:
                display(HTML("<h4>é‡å¤è®°å½•ç¤ºä¾‹</h4>"))
                
                for table in tables_with_duplicates:
                    if table in self.duplicate_analysis:
                        examples = self.duplicate_analysis[table]['examples']
                        duplicate_count = self.duplicate_analysis[table]['duplicate_count']
                        
                        display(Markdown(f"**è¡¨ {table} (å…± {duplicate_count} æ¡é‡å¤è®°å½•):**"))
                        display(pd.DataFrame(examples))
                
                # æä¾›å¤„ç†é‡å¤è®°å½•çš„å»ºè®®
                display(HTML("<h4>å¤„ç†é‡å¤è®°å½•çš„å»ºè®®</h4>"))
                display(Markdown("""
                **é‡å¤è®°å½•å¤„ç†å»ºè®®:**
                
                1. **åŸºäºä¸»é”®/å”¯ä¸€é”®çš„å»é‡:**
                   ```sql
                   -- ç¤ºä¾‹ï¼šä½¿ç”¨ROW_NUMBER()ä¿ç•™æœ€æ–°è®°å½•
                   WITH RankedRows AS (
                       SELECT *, 
                           ROW_NUMBER() OVER(PARTITION BY key_column ORDER BY updated_at DESC) as rn
                       FROM schema.table
                   )
                   DELETE FROM schema.table
                   WHERE id IN (
                       SELECT id FROM RankedRows WHERE rn > 1
                   );
                   ```
                
                2. **åˆ›å»ºå¸¦å”¯ä¸€çº¦æŸçš„ä¸´æ—¶è¡¨:**
                   ```sql
                   -- æ­¥éª¤1ï¼šåˆ›å»ºä¸´æ—¶è¡¨å¹¶å¸¦æœ‰å”¯ä¸€çº¦æŸ
                   CREATE TABLE temp_table (LIKE original_table);
                   ALTER TABLE temp_table ADD CONSTRAINT unique_key UNIQUE (key_column);
                   
                   -- æ­¥éª¤2ï¼šæ’å…¥ä¸é‡å¤çš„æ•°æ®
                   INSERT INTO temp_table
                   SELECT DISTINCT ON (key_column) *
                   FROM original_table
                   ORDER BY key_column, updated_at DESC;
                   
                   -- æ­¥éª¤3ï¼šæ›¿æ¢åŸè¡¨
                   DROP TABLE original_table;
                   ALTER TABLE temp_table RENAME TO original_table;
                   ```
                
                3. **æ·»åŠ æˆ–å¼ºåŒ–å”¯ä¸€çº¦æŸ:**
                   åœ¨å¤„ç†å®Œå½“å‰é‡å¤åï¼Œè€ƒè™‘æ·»åŠ å”¯ä¸€çº¦æŸä»¥é˜²æ­¢å°†æ¥å‡ºç°é‡å¤ã€‚
                """))
            else:
                display(Markdown("âœ… æœªåœ¨ä¸»è¦è¡¨ä¸­å‘ç°é‡å¤è®°å½•ã€‚"))
        
        except Exception as e:
            print(f"âŒ åˆ†æé‡å¤è®°å½•æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_data_consistency(self):
        """åˆ†ææ•°æ®ä¸€è‡´æ€§"""
        display(HTML("<h3>æ•°æ®ä¸€è‡´æ€§åˆ†æ</h3>"))
        
        try:
            # 1. æ£€æŸ¥æ—¥æœŸå­—æ®µçš„æœ‰æ•ˆæ€§
            display(HTML("<h4>æ—¥æœŸå­—æ®µæœ‰æ•ˆæ€§</h4>"))
            
            date_fields = {}
            
            # è·å–æ‰€æœ‰æ—¥æœŸç±»å‹çš„å­—æ®µ
            self.cur.execute(f"""
                SELECT 
                    table_name, 
                    column_name
                FROM 
                    information_schema.columns
                WHERE 
                    table_schema = '{self.schema}'
                    AND (data_type LIKE '%date%' OR data_type LIKE '%time%')
                ORDER BY
                    table_name, 
                    column_name
            """)
            
            date_columns = self.cur.fetchall()
            
            date_validation_results = []
            
            for col in date_columns:
                table = col['table_name']
                column = col['column_name']
                
                # è·³è¿‡å¾ˆå°çš„è¡¨
                if table in self.table_stats and self.table_stats[table]['row_count'] < 100:
                    continue
                
                # æ£€æŸ¥æ—¥æœŸçš„èŒƒå›´
                self.cur.execute(f"""
                    SELECT 
                        MIN({column}) as min_date,
                        MAX({column}) as max_date,
                        COUNT(*) as total_count,
                        SUM(CASE WHEN {column} IS NULL THEN 1 ELSE 0 END) as null_count,
                        SUM(CASE WHEN {column} > CURRENT_DATE THEN 1 ELSE 0 END) as future_date_count
                    FROM {self.schema}.{table}
                """)
                
                result = self.cur.fetchone()
                
                if result and result['total_count'] > 0:
                    # è®¡ç®—ç™¾åˆ†æ¯”
                    null_percentage = (result['null_count'] / result['total_count']) * 100 if result['total_count'] > 0 else 0
                    future_percentage = (result['future_date_count'] / result['total_count']) * 100 if result['total_count'] > 0 else 0
                    
                    date_validation_results.append({
                        'è¡¨å': table,
                        'åˆ—å': column,
                        'æœ€å°æ—¥æœŸ': result['min_date'],
                        'æœ€å¤§æ—¥æœŸ': result['max_date'],
                        'ç©ºå€¼ç™¾åˆ†æ¯”': round(null_percentage, 2),
                        'æœªæ¥æ—¥æœŸæ•°': result['future_date_count'],
                        'æœªæ¥æ—¥æœŸç™¾åˆ†æ¯”': round(future_percentage, 2)
                    })
                    
                    # å­˜å‚¨æ—¥æœŸå­—æ®µä¿¡æ¯
                    if table not in date_fields:
                        date_fields[table] = []
                    
                    date_fields[table].append({
                        'column': column,
                        'min_date': result['min_date'],
                        'max_date': result['max_date'],
                        'null_percentage': null_percentage,
                        'future_date_count': result['future_date_count']
                    })
            
            # æ˜¾ç¤ºæ—¥æœŸéªŒè¯ç»“æœ
            if date_validation_results:
                date_df = pd.DataFrame(date_validation_results)
                
                # æŸ¥æ‰¾é—®é¢˜æ—¥æœŸå­—æ®µ
                problem_dates_df = date_df[(date_df['æœªæ¥æ—¥æœŸæ•°'] > 0) | 
                                           (date_df['æœ€å°æ—¥æœŸ'] < '1900-01-01') |
                                           (date_df['ç©ºå€¼ç™¾åˆ†æ¯”'] > 80)]
                
                if not problem_dates_df.empty:
                    display(Markdown("**æ£€æµ‹åˆ°ä»¥ä¸‹æ—¥æœŸå­—æ®µå¯èƒ½å­˜åœ¨é—®é¢˜:**"))
                    display(problem_dates_df.sort_values(by=['æœªæ¥æ—¥æœŸæ•°', 'ç©ºå€¼ç™¾åˆ†æ¯”'], ascending=False))
                    
                    # æä¾›å»ºè®®
                    display(Markdown("""
                    **æ—¥æœŸå­—æ®µä¿®å¤å»ºè®®:**
                    
                    1. **æœªæ¥æ—¥æœŸå¤„ç†:**
                       - éªŒè¯æ˜¯å¦ä¸ºçœŸå®çš„æœªæ¥æ—¥æœŸï¼ˆé¢„æœŸæ—¥æœŸï¼‰æˆ–æ˜¯æ•°æ®é”™è¯¯
                       - è€ƒè™‘å°†æ˜æ˜¾é”™è¯¯çš„æœªæ¥æ—¥æœŸè®¾ç½®ä¸ºNULLæˆ–åˆç†çš„é»˜è®¤å€¼
                       
                    2. **å¼‚å¸¸æ—©æœŸæ—¥æœŸ:**
                       - æ£€æŸ¥1900å¹´ä¹‹å‰çš„æ—¥æœŸæ˜¯å¦åˆç†
                       - å¯èƒ½éœ€è¦è®¾ç½®ä¸ºNULLæˆ–æ›´åˆç†çš„æ—¥æœŸ
                       
                    3. **æ—¥æœŸéªŒè¯SQLç¤ºä¾‹:**
                       ```sql
                       -- æŸ¥æ‰¾æœªæ¥æ—¥æœŸ
                       SELECT * FROM schema.table WHERE date_column > CURRENT_DATE;
                       
                       -- ä¿®å¤æ˜æ˜¾é”™è¯¯çš„æ—¥æœŸ
                       UPDATE schema.table 
                       SET date_column = NULL
                       WHERE date_column > CURRENT_DATE + INTERVAL '1 year';
                       ```
                    """))
                else:
                    display(Markdown("âœ… æ—¥æœŸå­—æ®µéªŒè¯é€šè¿‡ï¼Œæœªå‘ç°æ˜æ˜¾å¼‚å¸¸ã€‚"))
            
            # 2. æ£€æŸ¥æ•°å€¼å­—æ®µçš„æœ‰æ•ˆæ€§
            display(HTML("<h4>æ•°å€¼å­—æ®µæœ‰æ•ˆæ€§</h4>"))
            
            # è·å–æ•°å€¼ç±»å‹å­—æ®µ
            self.cur.execute(f"""
                SELECT 
                    table_name, 
                    column_name,
                    data_type
                FROM 
                    information_schema.columns
                WHERE 
                    table_schema = '{self.schema}'
                    AND (
                        data_type LIKE '%int%' OR 
                        data_type LIKE '%float%' OR
                        data_type LIKE '%double%' OR
                        data_type LIKE '%numeric%' OR
                        data_type LIKE '%decimal%'
                    )
                ORDER BY
                    table_name, 
                    column_name
            """)
            
            numeric_columns = self.cur.fetchall()
            
            numeric_validation_results = []
            
            for col in numeric_columns:
                table = col['table_name']
                column = col['column_name']
                data_type = col['data_type']
                
                # è·³è¿‡IDåˆ—å’Œå¾ˆå°çš„è¡¨
                if column in ('id', 'event_id', 'report_id', 'device_id') or \
                   (table in self.table_stats and self.table_stats[table]['row_count'] < 100):
                    continue
                
                # æ£€æŸ¥æ•°å€¼èŒƒå›´å’Œåˆ†å¸ƒ
                self.cur.execute(f"""
                    SELECT 
                        MIN({column}) as min_value,
                        MAX({column}) as max_value,
                        AVG({column}) as avg_value,
                        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY {column}) as median_value,
                        COUNT(*) as total_count,
                        SUM(CASE WHEN {column} IS NULL THEN 1 ELSE 0 END) as null_count,
                        SUM(CASE WHEN {column} < 0 THEN 1 ELSE 0 END) as negative_count,
                        SUM(CASE WHEN {column} = 0 THEN 1 ELSE 0 END) as zero_count
                    FROM {self.schema}.{table}
                """)
                
                result = self.cur.fetchone()
                
                if result and result['total_count'] > 0 and result['total_count'] - result['null_count'] > 0:
                    # è®¡ç®—ç»Ÿè®¡å€¼
                    null_percentage = (result['null_count'] / result['total_count']) * 100 if result['total_count'] > 0 else 0
                    negative_percentage = (result['negative_count'] / (result['total_count'] - result['null_count'])) * 100 \
                                          if (result['total_count'] - result['null_count']) > 0 else 0
                    zero_percentage = (result['zero_count'] / (result['total_count'] - result['null_count'])) * 100 \
                                      if (result['total_count'] - result['null_count']) > 0 else 0
                    
                    # è®¡ç®—æœ€å¤§å€¼ä¸å¹³å‡å€¼çš„æ¯”ç‡ï¼Œæ£€æµ‹å¼‚å¸¸å€¼
                    if result['avg_value'] and result['avg_value'] != 0:
                        max_avg_ratio = result['max_value'] / result['avg_value'] if result['max_value'] else 0
                    else:
                        max_avg_ratio = 0
                    
                    numeric_validation_results.append({
                        'è¡¨å': table,
                        'åˆ—å': column,
                        'æ•°æ®ç±»å‹': data_type,
                        'æœ€å°å€¼': result['min_value'],
                        'æœ€å¤§å€¼': result['max_value'],
                        'å¹³å‡å€¼': round(result['avg_value'], 2) if result['avg_value'] else None,
                        'ä¸­ä½æ•°': round(result['median_value'], 2) if result['median_value'] else None,
                        'ç©ºå€¼ç™¾åˆ†æ¯”': round(null_percentage, 2),
                        'è´Ÿå€¼ç™¾åˆ†æ¯”': round(negative_percentage, 2),
                        'é›¶å€¼ç™¾åˆ†æ¯”': round(zero_percentage, 2),
                        'æœ€å¤§/å¹³å‡æ¯”': round(max_avg_ratio, 2) if max_avg_ratio else None
                    })
            
            # æ˜¾ç¤ºæ•°å€¼éªŒè¯ç»“æœ
            if numeric_validation_results:
                numeric_df = pd.DataFrame(numeric_validation_results)
                
                # æŸ¥æ‰¾å¯èƒ½æœ‰é—®é¢˜çš„æ•°å€¼å­—æ®µ
                problem_numeric_df = numeric_df[
                    (numeric_df['æœ€å¤§/å¹³å‡æ¯”'] > 100) |  # å¯èƒ½å­˜åœ¨å¼‚å¸¸å€¼
                    ((numeric_df['è´Ÿå€¼ç™¾åˆ†æ¯”'] > 0) & (numeric_df['åˆ—å'].str.contains('count|amount|quantity|number', case=False))) |  # ä¸åº”è¯¥ä¸ºè´Ÿçš„å­—æ®µæœ‰è´Ÿå€¼
                    (numeric_df['ç©ºå€¼ç™¾åˆ†æ¯”'] > 80)  # å¤§å¤šæ•°ä¸ºç©º
                ]
                
                if not problem_numeric_df.empty:
                    display(Markdown("**æ£€æµ‹åˆ°ä»¥ä¸‹æ•°å€¼å­—æ®µå¯èƒ½å­˜åœ¨é—®é¢˜:**"))
                    display(problem_numeric_df.sort_values(by=['æœ€å¤§/å¹³å‡æ¯”', 'è´Ÿå€¼ç™¾åˆ†æ¯”'], ascending=False))
                    
                    # æä¾›å»ºè®®
                    display(Markdown("""
                    **æ•°å€¼å­—æ®µä¿®å¤å»ºè®®:**
                    
                    1. **å¼‚å¸¸å€¼å¤„ç†:**
                       - å¯¹äºæœ€å¤§å€¼è¿œå¤§äºå¹³å‡å€¼çš„å­—æ®µï¼Œåº”æ£€æŸ¥æ˜¯å¦å­˜åœ¨æç«¯å€¼
                       - è€ƒè™‘ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•ï¼ˆå¦‚Z-scoreæˆ–IQRæ–¹æ³•ï¼‰è¯†åˆ«å’Œå¤„ç†å¼‚å¸¸å€¼
                       
                    2. **è´Ÿå€¼å¤„ç†:**
                       - å¯¹äºåç§°æš—ç¤ºåº”ä¸ºæ­£å€¼çš„å­—æ®µï¼ˆå¦‚è®¡æ•°ã€æ•°é‡ã€é‡‘é¢ï¼‰ï¼Œæ£€æŸ¥è´Ÿå€¼
                       - æ ¹æ®ä¸šåŠ¡è§„åˆ™ç¡®å®šæ˜¯å¦éœ€è¦å°†è´Ÿå€¼è½¬æ¢ä¸º0æˆ–NULL
                       
                    3. **æ•°å€¼æ£€éªŒSQLç¤ºä¾‹:**
                       ```sql
                       -- æŸ¥æ‰¾å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
                       WITH Stats AS (
                           SELECT 
                               percentile_cont(0.25) WITHIN GROUP (ORDER BY numeric_column) AS q1,
                               percentile_cont(0.75) WITHIN GROUP (ORDER BY numeric_column) AS q3
                           FROM schema.table
                       )
                       SELECT *
                       FROM schema.table, Stats
                       WHERE numeric_column > (q3 + (q3 - q1) * 1.5)
                          OR numeric_column < (q1 - (q3 - q1) * 1.5);
                       ```
                    """))
                else:
                    display(Markdown("âœ… æ•°å€¼å­—æ®µéªŒè¯é€šè¿‡ï¼Œæœªå‘ç°æ˜æ˜¾å¼‚å¸¸ã€‚"))
            
            # 3. æ£€æŸ¥æšä¸¾/ä»£ç å€¼çš„ä¸€è‡´æ€§
            display(HTML("<h4>æšä¸¾/ä»£ç å€¼ä¸€è‡´æ€§</h4>"))
            
            # å¯èƒ½çš„æšä¸¾å­—æ®µåˆ—è¡¨ï¼ˆåŸºäºåˆ—åæ¨¡å¼ï¼‰
            enum_patterns = [
                '%_type', '%_code', '%_status', '%_flag', 'classification', 
                'device_class', 'category', '%_level'
            ]
            
            potential_enum_fields = []
            
            for pattern in enum_patterns:
                self.cur.execute(f"""
                    SELECT 
                        table_name, 
                        column_name,
                        data_type
                    FROM 
                        information_schema.columns
                    WHERE 
                        table_schema = '{self.schema}'
                        AND column_name LIKE '{pattern}'
                        AND data_type NOT IN ('boolean', 'uuid', 'bytea')
                    ORDER BY
                        table_name, 
                        column_name
                """)
                
                potential_enum_fields.extend(self.cur.fetchall())
            
            enum_validation_results = []
            
            for col in potential_enum_fields:
                table = col['table_name']
                column = col['column_name']
                data_type = col['data_type']
                
                # è·³è¿‡å°è¡¨
                if table in self.table_stats and self.table_stats[table]['row_count'] < 100:
                    continue
                
                # è·å–ä¸åŒå€¼åŠå…¶è®¡æ•°
                self.cur.execute(f"""
                    SELECT 
                        {column} as value,
                        COUNT(*) as count
                    FROM 
                        {self.schema}.{table}
                    WHERE 
                        {column} IS NOT NULL
                    GROUP BY 
                        {column}
                    ORDER BY 
                        COUNT(*) DESC
                """)
                
                value_counts = self.cur.fetchall()
                
                if value_counts:
                    # è®¡ç®—ä¸åŒå€¼çš„æ•°é‡
                    distinct_values = len(value_counts)
                    
                    # è®¡ç®—æœ€å¸¸è§å€¼å æ¯”
                    top_value = value_counts[0]['value'] if value_counts else None
                    top_count = value_counts[0]['count'] if value_counts else 0
                    
                    self.cur.execute(f"""
                        SELECT COUNT(*) as total
                        FROM {self.schema}.{table}
                        WHERE {column} IS NOT NULL
                    """)
                    
                    total_result = self.cur.fetchone()
                    total_count = total_result['total'] if total_result else 0
                    
                    top_percentage = (top_count / total_count * 100) if total_count > 0 else 0
                    
                    enum_validation_results.append({
                        'è¡¨å': table,
                        'åˆ—å': column,
                        'æ•°æ®ç±»å‹': data_type,
                        'ä¸åŒå€¼æ•°é‡': distinct_values,
                        'æœ€å¸¸è§å€¼': str(top_value)[:50] if top_value is not None else None,  # æˆªæ–­é•¿å€¼
                        'æœ€å¸¸è§å€¼å æ¯”': round(top_percentage, 2),
                        'å€¼åˆ†å¸ƒ': [
                            f"{row['value']}: {row['count']}" 
                            for row in value_counts[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªæœ€å¸¸è§çš„å€¼
                        ]
                    })
            
            # æ˜¾ç¤ºæšä¸¾éªŒè¯ç»“æœ
            if enum_validation_results:
                enum_df = pd.DataFrame(enum_validation_results)
                
                # è¯†åˆ«å¯èƒ½çš„é—®é¢˜å­—æ®µ
                nonstandard_enums = enum_df[enum_df['ä¸åŒå€¼æ•°é‡'] > 20].sort_values(by='ä¸åŒå€¼æ•°é‡', ascending=False)
                
                if not nonstandard_enums.empty:
                    display(Markdown("**ä»¥ä¸‹ä»£ç /æšä¸¾å­—æ®µå¯èƒ½å­˜åœ¨æ ‡å‡†åŒ–é—®é¢˜:**"))
                    display(nonstandard_enums[['è¡¨å', 'åˆ—å', 'ä¸åŒå€¼æ•°é‡', 'æœ€å¸¸è§å€¼', 'æœ€å¸¸è§å€¼å æ¯”']])
                    
                    # æ˜¾ç¤ºä¸€äº›ç‰¹å®šå­—æ®µçš„è¯¦ç»†å€¼åˆ†å¸ƒ
                    if len(nonstandard_enums) > 0:
                        for i, row in nonstandard_enums.head(3).iterrows():
                            table = row['è¡¨å']
                            column = row['åˆ—å']
                            
                            display(Markdown(f"**{table}.{column} å€¼åˆ†å¸ƒ:**"))
                            
                            self.cur.execute(f"""
                                SELECT 
                                    {column} as value,
                                    COUNT(*) as count,
                                    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM {self.schema}.{table} WHERE {column} IS NOT NULL), 2) as percentage
                                FROM 
                                    {self.schema}.{table}
                                WHERE 
                                    {column} IS NOT NULL
                                GROUP BY 
                                    {column}
                                ORDER BY 
                                    COUNT(*) DESC
                                LIMIT 15
                            """)
                            
                            detailed_counts = self.cur.fetchall()
                            display(pd.DataFrame(detailed_counts))
                    
                    # æä¾›æ ‡å‡†åŒ–å»ºè®®
                    display(Markdown("""
                    **ä»£ç /æšä¸¾å­—æ®µæ ‡å‡†åŒ–å»ºè®®:**
                    
                    1. **æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–:**
                       - å¤„ç†å¤§å°å†™ä¸ä¸€è‡´ï¼ˆå¯ä»¥å…¨éƒ¨è½¬ä¸ºå¤§å†™æˆ–å°å†™ï¼‰
                       - åˆ é™¤å¤šä½™çš„ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·ç­‰
                       - åˆå¹¶ç›¸ä¼¼æˆ–ç­‰ä»·çš„å€¼ï¼ˆä¾‹å¦‚"Y"/"YES"/"1"ç»Ÿä¸€ä¸º"Y"ï¼‰
                       
                    2. **åˆ›å»ºæ ‡å‡†ä»£ç è¡¨:**
                       - ä¸ºé‡è¦çš„æšä¸¾å­—æ®µåˆ›å»ºæŸ¥æ‰¾è¡¨
                       - ä½¿ç”¨å¤–é”®å¼ºåˆ¶å¼•ç”¨æœ‰æ•ˆå€¼
                       
                    3. **æ ‡å‡†åŒ–SQLç¤ºä¾‹:**
                       ```sql
                       -- å¤§å°å†™æ ‡å‡†åŒ–ç¤ºä¾‹
                       UPDATE schema.table
                       SET code_column = UPPER(TRIM(code_column))
                       WHERE code_column IS NOT NULL;
                       
                       -- åˆ›å»ºæ ‡å‡†ä»£ç è¡¨ç¤ºä¾‹
                       CREATE TABLE schema.standard_codes (
                           code VARCHAR(50) PRIMARY KEY,
                           description TEXT,
                           is_active BOOLEAN DEFAULT TRUE
                       );
                       ```
                    """))
                else:
                    display(Markdown("âœ… ä»£ç /æšä¸¾å­—æ®µéªŒè¯é€šè¿‡ï¼Œæœªå‘ç°æ˜æ˜¾æ ‡å‡†åŒ–é—®é¢˜ã€‚"))
        
        except Exception as e:
            print(f"âŒ åˆ†ææ•°æ®ä¸€è‡´æ€§æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_column_statistics(self):
        """åˆ†æå…³é”®åˆ—çš„ç»Ÿè®¡ä¿¡æ¯"""
        display(HTML("<h3>å…³é”®åˆ—ç»Ÿè®¡åˆ†æ</h3>"))
        
        try:
            # é€‰æ‹©è¦åˆ†æçš„å…³é”®è¡¨å’Œåˆ—
            key_columns = {
                'adverse_events': ['date_received', 'date_of_event', 'event_type', 'health_professional'],
                'device_recalls': ['event_date_initiated', 'classification', 'product_code'],
                'product_codes': ['device_class', 'implant_flag', 'life_sustain_support_flag'],
                'udi_records': ['publish_date', 'is_single_use', 'is_rx', 'is_kit']
            }
            
            all_stats = []
            
            for table, columns in key_columns.items():
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                if table not in self.table_stats:
                    continue
                
                for column in columns:
                    # æŸ¥æ‰¾åˆ—çš„æ•°æ®ç±»å‹
                    self.cur.execute(f"""
                        SELECT data_type
                        FROM information_schema.columns
                        WHERE table_schema = '{self.schema}'
                        AND table_name = '{table}'
                        AND column_name = '{column}'
                    """)
                    
                    column_type_result = self.cur.fetchone()
                    
                    if not column_type_result:
                        continue
                        
                    data_type = column_type_result['data_type']
                    
                    # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡
                    if data_type in ('integer', 'numeric', 'decimal', 'double precision', 'real'):
                        # æ•°å€¼å‹ç»Ÿè®¡
                        self.cur.execute(f"""
                            SELECT 
                                MIN({column}) as min_value,
                                MAX({column}) as max_value,
                                AVG({column}) as avg_value,
                                PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY {column}) as median,
                                STDDEV({column}) as std_dev,
                                COUNT(*) as total_count,
                                COUNT({column}) as non_null_count
                            FROM {self.schema}.{table}
                        """)
                        
                        result = self.cur.fetchone()
                        
                        if result and result['total_count'] > 0:
                            stats = {
                                'è¡¨å': table,
                                'åˆ—å': column,
                                'æ•°æ®ç±»å‹': 'æ•°å€¼å‹',
                                'æ€»è®°å½•æ•°': result['total_count'],
                                'éç©ºè®°å½•æ•°': result['non_null_count'],
                                'æœ€å°å€¼': result['min_value'],
                                'æœ€å¤§å€¼': result['max_value'],
                                'å¹³å‡å€¼': round(result['avg_value'], 2) if result['avg_value'] else None,
                                'ä¸­ä½æ•°': round(result['median'], 2) if result['median'] else None,
                                'æ ‡å‡†å·®': round(result['std_dev'], 2) if result['std_dev'] else None
                            }
                            all_stats.append(stats)
                            
                    elif data_type in ('date', 'timestamp', 'timestamp without time zone', 'timestamp with time zone'):
                        # æ—¥æœŸå‹ç»Ÿè®¡
                        self.cur.execute(f"""
                            SELECT 
                                MIN({column}) as min_date,
                                MAX({column}) as max_date,
                                COUNT(*) as total_count,
                                COUNT({column}) as non_null_count
                            FROM {self.schema}.{table}
                        """)
                        
                        result = self.cur.fetchone()
                        
                        if result and result['total_count'] > 0:
                            # è®¡ç®—å¹´åº¦åˆ†å¸ƒ
                            self.cur.execute(f"""
                                SELECT 
                                    EXTRACT(YEAR FROM {column}) as year,
                                    COUNT(*) as count
                                FROM {self.schema}.{table}
                                WHERE {column} IS NOT NULL
                                GROUP BY EXTRACT(YEAR FROM {column})
                                ORDER BY year DESC
                                LIMIT 5
                            """)
                            
                            year_distribution = self.cur.fetchall()
                            
                            stats = {
                                'è¡¨å': table,
                                'åˆ—å': column,
                                'æ•°æ®ç±»å‹': 'æ—¥æœŸå‹',
                                'æ€»è®°å½•æ•°': result['total_count'],
                                'éç©ºè®°å½•æ•°': result['non_null_count'],
                                'æœ€æ—©æ—¥æœŸ': result['min_date'],
                                'æœ€æ™šæ—¥æœŸ': result['max_date'],
                                'å¹´åº¦åˆ†å¸ƒ': [f"{row['year']}: {row['count']}" for row in year_distribution]
                            }
                            all_stats.append(stats)
                            
                    elif data_type in ('boolean'):
                        # å¸ƒå°”å‹ç»Ÿè®¡
                        self.cur.execute(f"""
                            SELECT 
                                COUNT(*) as total_count,
                                COUNT({column}) as non_null_count,
                                SUM(CASE WHEN {column} = TRUE THEN 1 ELSE 0 END) as true_count,
                                SUM(CASE WHEN {column} = FALSE THEN 1 ELSE 0 END) as false_count
                            FROM {self.schema}.{table}
                        """)
                        
                        result = self.cur.fetchone()
                        
                        if result and result['total_count'] > 0:
                            true_percentage = (result['true_count'] / result['non_null_count'] * 100) if result['non_null_count'] > 0 else 0
                            stats = {
                                'è¡¨å': table,
                                'åˆ—å': column,
                                'æ•°æ®ç±»å‹': 'å¸ƒå°”å‹',
                                'æ€»è®°å½•æ•°': result['total_count'],
                                'éç©ºè®°å½•æ•°': result['non_null_count'],
                                'TRUEå€¼æ•°': result['true_count'],
                                'FALSEå€¼æ•°': result['false_count'],
                                'TRUEå æ¯”': f"{round(true_percentage, 2)}%"
                            }
                            all_stats.append(stats)
                            
                    else:
                        # å­—ç¬¦ä¸²/åˆ†ç±»å‹ç»Ÿè®¡
                        self.cur.execute(f"""
                            SELECT 
                                COUNT(*) as total_count,
                                COUNT({column}) as non_null_count,
                                COUNT(DISTINCT {column}) as distinct_count
                            FROM {self.schema}.{table}
                        """)
                        
                        result = self.cur.fetchone()
                        
                        if result and result['total_count'] > 0:
                            # è·å–å‰5ä¸ªæœ€å¸¸è§å€¼
                            self.cur.execute(f"""
                                SELECT 
                                    {column} as value,
                                    COUNT(*) as count,
                                    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM {self.schema}.{table} WHERE {column} IS NOT NULL), 2) as percentage
                                FROM {self.schema}.{table}
                                WHERE {column} IS NOT NULL
                                GROUP BY {column}
                                ORDER BY COUNT(*) DESC
                                LIMIT 5
                            """)
                            
                            top_values = self.cur.fetchall()
                            
                            stats = {
                                'è¡¨å': table,
                                'åˆ—å': column,
                                'æ•°æ®ç±»å‹': 'å­—ç¬¦ä¸²/åˆ†ç±»å‹',
                                'æ€»è®°å½•æ•°': result['total_count'],
                                'éç©ºè®°å½•æ•°': result['non_null_count'],
                                'ä¸åŒå€¼æ•°é‡': result['distinct_count'],
                                'æœ€å¸¸è§å€¼': [f"{row['value']}: {row['count']} ({row['percentage']}%)" for row in top_values]
                            }
                            all_stats.append(stats)
            
            # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
            if all_stats:
                display(Markdown("**å…³é”®åˆ—ç»Ÿè®¡åˆ†æ:**"))
                
                # åˆ†ç»„æ˜¾ç¤ºä¸åŒç±»å‹çš„ç»Ÿè®¡
                data_types = set(stat['æ•°æ®ç±»å‹'] for stat in all_stats)
                
                for data_type in data_types:
                    display(Markdown(f"**{data_type}å­—æ®µ:**"))
                    
                    type_stats = [stat for stat in all_stats if stat['æ•°æ®ç±»å‹'] == data_type]
                    if data_type == 'æ•°å€¼å‹':
                        df = pd.DataFrame(type_stats)[['è¡¨å', 'åˆ—å', 'éç©ºè®°å½•æ•°', 'æœ€å°å€¼', 'æœ€å¤§å€¼', 'å¹³å‡å€¼', 'ä¸­ä½æ•°', 'æ ‡å‡†å·®']]
                    elif data_type == 'æ—¥æœŸå‹':
                        df = pd.DataFrame(type_stats)[['è¡¨å', 'åˆ—å', 'éç©ºè®°å½•æ•°', 'æœ€æ—©æ—¥æœŸ', 'æœ€æ™šæ—¥æœŸ']]
                        # æ˜¾ç¤ºå¹´åº¦åˆ†å¸ƒ
                        for i, row in enumerate(type_stats):
                            if 'å¹´åº¦åˆ†å¸ƒ' in row and row['å¹´åº¦åˆ†å¸ƒ']:
                                display(Markdown(f"_{row['è¡¨å']}.{row['åˆ—å']} å¹´åº¦åˆ†å¸ƒ:_ {', '.join(row['å¹´åº¦åˆ†å¸ƒ'])}"))
                    elif data_type == 'å¸ƒå°”å‹':
                        df = pd.DataFrame(type_stats)[['è¡¨å', 'åˆ—å', 'éç©ºè®°å½•æ•°', 'TRUEå€¼æ•°', 'FALSEå€¼æ•°', 'TRUEå æ¯”']]
                    else:  # å­—ç¬¦ä¸²/åˆ†ç±»å‹
                        df = pd.DataFrame(type_stats)[['è¡¨å', 'åˆ—å', 'éç©ºè®°å½•æ•°', 'ä¸åŒå€¼æ•°é‡']]
                        # æ˜¾ç¤ºæœ€å¸¸è§å€¼
                        for i, row in enumerate(type_stats):
                            if 'æœ€å¸¸è§å€¼' in row and row['æœ€å¸¸è§å€¼']:
                                display(Markdown(f"_{row['è¡¨å']}.{row['åˆ—å']} æœ€å¸¸è§å€¼:_ {'; '.join(row['æœ€å¸¸è§å€¼'])}"))
                    
                    display(df)
                
                # ä¸ºä¸€äº›æœ‰è¶£çš„ç»Ÿè®¡åˆ¶ä½œå¯è§†åŒ–
                boolean_stats = [stat for stat in all_stats if stat['æ•°æ®ç±»å‹'] == 'å¸ƒå°”å‹' and stat['éç©ºè®°å½•æ•°'] > 100]
                
                if boolean_stats:
                    plt.figure(figsize=(12, 6))
                    
                    # åˆ›å»ºå¸ƒå°”å­—æ®µåˆ†å¸ƒæ¡å½¢å›¾
                    labels = [f"{stat['è¡¨å']}.{stat['åˆ—å']}" for stat in boolean_stats]
                    true_vals = [stat['TRUEå€¼æ•°'] for stat in boolean_stats]
                    false_vals = [stat['FALSEå€¼æ•°'] for stat in boolean_stats]
                    
                    x = range(len(labels))
                    width = 0.35
                    
                    fig, ax = plt.subplots(figsize=(14, 7))
                    rects1 = ax.bar([i - width/2 for i in x], true_vals, width, label='True')
                    rects2 = ax.bar([i + width/2 for i in x], false_vals, width, label='False')
                    
                    ax.set_title('å¸ƒå°”å­—æ®µå€¼åˆ†å¸ƒ')
                    ax.set_xticks(x)
                    ax.set_xticklabels(labels, rotation=45, ha='right')
                    ax.legend()
                    
                    plt.tight_layout()
                    plt.show()
            
            # å­˜å‚¨è¿™äº›ç»Ÿè®¡ç”¨äºåç»­åˆ†æ
            self.column_stats = {f"{stat['è¡¨å']}.{stat['åˆ—å']}": stat for stat in all_stats}
        
        except Exception as e:
            print(f"âŒ åˆ†æåˆ—ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_categorical_distributions(self):
        """åˆ†æåˆ†ç±»å€¼çš„åˆ†å¸ƒ"""
        display(HTML("<h3>åˆ†ç±»å€¼åˆ†å¸ƒåˆ†æ</h3>"))
        
        try:
            # é€‰æ‹©è¦åˆ†æçš„åˆ†ç±»åˆ—
            categorical_columns = {
                'device_recalls': 'classification',
                'adverse_events': 'event_type',
                'product_codes': 'device_class',
                'udi_records': 'mri_safety'
            }
            
            for table, column in categorical_columns.items():
                # æ£€æŸ¥è¡¨å’Œåˆ—æ˜¯å¦å­˜åœ¨
                self.cur.execute(f"""
                    SELECT 1
                    FROM information_schema.columns
                    WHERE table_schema = '{self.schema}'
                    AND table_name = '{table}'
                    AND column_name = '{column}'
                """)
                
                if not self.cur.fetchone():
                    continue
                
                display(Markdown(f"**{table}.{column} å€¼åˆ†å¸ƒ:**"))
                
                # è·å–å€¼åˆ†å¸ƒ
                self.cur.execute(f"""
                    SELECT 
                        {column} as value,
                        COUNT(*) as count,
                        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
                    FROM {self.schema}.{table}
                    WHERE {column} IS NOT NULL
                    GROUP BY {column}
                    ORDER BY COUNT(*) DESC
                """)
                
                distribution = self.cur.fetchall()
                
                if distribution:
                    df = pd.DataFrame(distribution)
                    display(df)
                    
                    # åˆ›å»ºé¥¼å›¾æˆ–æ¡å½¢å›¾
                    plt.figure(figsize=(10, 6))
                    
                    # å¦‚æœå€¼å¤ªå¤šï¼Œåªæ˜¾ç¤ºå‰10ä¸ª
                    if len(distribution) > 10:
                        top_values = df.head(10)
                        others_sum = df.iloc[10:]['count'].sum()
                        others_pct = df.iloc[10:]['percentage'].sum()
                        
                        # æ·»åŠ "å…¶ä»–"ç±»åˆ«
                        if others_sum > 0:
                            top_values = pd.concat([
                                top_values, 
                                pd.DataFrame([{'value': 'å…¶ä»–', 'count': others_sum, 'percentage': others_pct}])
                            ])
                        
                        plt.bar(top_values['value'].astype(str), top_values['count'])
                        plt.title(f'{table}.{column} å€¼åˆ†å¸ƒ (å‰10å)')
                        plt.xticks(rotation=45, ha='right')
                    else:
                        # å¦‚æœä¸åŒå€¼å°‘äº5ä¸ªï¼Œä½¿ç”¨é¥¼å›¾
                        if len(distribution) <= 5:
                            plt.pie(df['count'], labels=df['value'].astype(str), autopct='%1.1f%%')
                            plt.title(f'{table}.{column} å€¼åˆ†å¸ƒ')
                        else:
                            plt.bar(df['value'].astype(str), df['count'])
                            plt.title(f'{table}.{column} å€¼åˆ†å¸ƒ')
                            plt.xticks(rotation=45, ha='right')
                    
                    plt.tight_layout()
                    plt.show()
        
        except Exception as e:
            print(f"âŒ åˆ†æåˆ†ç±»å€¼åˆ†å¸ƒæ—¶å‡ºé”™: {str(e)}")
    
    def analyze_time_series_patterns(self):
        """åˆ†ææ—¶é—´åºåˆ—æ¨¡å¼"""
        display(HTML("<h3>æ—¶é—´åºåˆ—æ¨¡å¼åˆ†æ</h3>"))
        
        try:
            # å®šä¹‰è¦åˆ†æçš„æ—¶é—´åºåˆ—
            time_series = {
                'adverse_events': {'date_column': 'date_received', 'count_column': 'report_number', 'group_by': 'event_type'},
                'device_recalls': {'date_column': 'event_date_initiated', 'count_column': 'recall_number', 'group_by': 'classification'}
            }
            
            for table, config in time_series.items():
                date_column = config['date_column']
                count_column = config['count_column']
                group_by = config.get('group_by')
                
                # æ£€æŸ¥è¡¨å’Œåˆ—æ˜¯å¦å­˜åœ¨
                self.cur.execute(f"""
                    SELECT 1
                    FROM information_schema.columns
                    WHERE table_schema = '{self.schema}'
                    AND table_name = '{table}'
                    AND column_name = '{date_column}'
                """)
                
                if not self.cur.fetchone():
                    continue
                
                # æŒ‰æœˆæ±‡æ€»æ•°æ®
                if group_by:
                    # åˆ†ç»„æ—¶é—´åºåˆ—
                    self.cur.execute(f"""
                        SELECT 
                            TO_CHAR(DATE_TRUNC('month', {date_column}), 'YYYY-MM') as month,
                            {group_by} as category,
                            COUNT({count_column}) as count
                        FROM {self.schema}.{table}
                        WHERE {date_column} IS NOT NULL
                        AND {date_column} >= '2000-01-01'
                        AND {date_column} <= CURRENT_DATE
                        AND {group_by} IS NOT NULL
                        GROUP BY month, {group_by}
                        ORDER BY month, {group_by}
                    """)
                    
                    results = self.cur.fetchall()
                    
                    if results:
                        df = pd.DataFrame(results)
                        
                        # æ‰¾å‡ºä¸»è¦ç±»åˆ«ï¼ˆæ’é™¤ç½•è§ç±»åˆ«ä»¥é¿å…å›¾è¡¨è¿‡äºå¤æ‚ï¼‰
                        main_categories = df['category'].value_counts().head(5).index.tolist()
                        df_filtered = df[df['category'].isin(main_categories)]
                        
                        # é€è§†è¡¨ä»¥ä¾¿ç»˜å›¾
                        pivot_df = df_filtered.pivot(index='month', columns='category', values='count')
                        
                        # ç»˜åˆ¶éšæ—¶é—´å˜åŒ–çš„ç±»åˆ«è¶‹åŠ¿
                        plt.figure(figsize=(14, 7))
                        pivot_df.plot(figsize=(14, 7), title=f'{table} æŒ‰ {group_by} åˆ†ç±»çš„æœˆåº¦è¶‹åŠ¿')
                        plt.xticks(rotation=45)
                        plt.grid(True, alpha=0.3)
                        plt.tight_layout()
                        plt.show()
                        
                        # è®¡ç®—åŒæ¯”å¢é•¿ç‡
                        display(Markdown(f"**{table} æŒ‰ {group_by} åˆ†ç±»çš„å¹´åº¦æ€»æ•°:**"))
                        
                        self.cur.execute(f"""
                            SELECT 
                                EXTRACT(YEAR FROM {date_column}) as year,
                                {group_by} as category,
                                COUNT({count_column}) as count
                            FROM {self.schema}.{table}
                            WHERE {date_column} IS NOT NULL
                            AND {date_column} >= '2000-01-01'
                            AND {date_column} <= CURRENT_DATE
                            AND {group_by} IS NOT NULL
                            GROUP BY year, {group_by}
                            ORDER BY year, {group_by}
                        """)
                        
                        yearly_results = self.cur.fetchall()
                        
                        if yearly_results:
                            yearly_df = pd.DataFrame(yearly_results)
                            
                            # åªä¿ç•™ä¸»è¦ç±»åˆ«
                            yearly_df = yearly_df[yearly_df['category'].isin(main_categories)]
                            
                            yearly_pivot = yearly_df.pivot(index='year', columns='category', values='count')
                            display(yearly_pivot)
                            
                            # è®¡ç®—åŒæ¯”å˜åŒ–
                            display(Markdown(f"**{table} æŒ‰ {group_by} åˆ†ç±»çš„åŒæ¯”å¢é•¿ç‡ (%):**"))
                            display(yearly_pivot.pct_change() * 100)
                else:
                    # ç®€å•æ—¶é—´åºåˆ—
                    self.cur.execute(f"""
                        SELECT 
                            TO_CHAR(DATE_TRUNC('month', {date_column}), 'YYYY-MM') as month,
                            COUNT({count_column}) as count
                        FROM {self.schema}.{table}
                        WHERE {date_column} IS NOT NULL
                        AND {date_column} >= '2000-01-01'
                        AND {date_column} <= CURRENT_DATE
                        GROUP BY month
                        ORDER BY month
                    """)
                    
                    results = self.cur.fetchall()
                    
                    if results:
                        df = pd.DataFrame(results)
                        
                        # ç»˜åˆ¶ç®€å•æ—¶é—´åºåˆ—
                        plt.figure(figsize=(14, 7))
                        plt.plot(df['month'], df['count'])
                        plt.title(f'{table} æŒ‰æœˆè®°å½•æ•°')
                        plt.xticks(rotation=45)
                        plt.grid(True, alpha=0.3)
                        plt.tight_layout()
                        plt.show()
                        
                        # è®¡ç®—å¹´åº¦æ±‡æ€»
                        display(Markdown(f"**{table} å¹´åº¦è®°å½•æ•°:**"))
                        
                        self.cur.execute(f"""
                            SELECT 
                                EXTRACT(YEAR FROM {date_column}) as year,
                                COUNT({count_column}) as count
                            FROM {self.schema}.{table}
                            WHERE {date_column} IS NOT NULL
                            AND {date_column} >= '2000-01-01'
                            AND {date_column} <= CURRENT_DATE
                            GROUP BY year
                            ORDER BY year
                        """)
                        
                        yearly_results = self.cur.fetchall()
                        
                        if yearly_results:
                            yearly_df = pd.DataFrame(yearly_results)
                            display(yearly_df)
                            
                            # è®¡ç®—åŒæ¯”å¢é•¿ç‡
                            yearly_df['å¢é•¿ç‡%'] = yearly_df['count'].pct_change() * 100
                            display(yearly_df)
        
        except Exception as e:
            print(f"âŒ åˆ†ææ—¶é—´åºåˆ—æ¨¡å¼æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_table_relationships(self):
        """åˆ†æè¡¨é—´å…³ç³»"""
        display(HTML("<h3>è¡¨é—´å…³ç³»åˆ†æ</h3>"))
        
        try:
            # æŸ¥æ‰¾å¤–é”®å…³ç³»
            self.cur.execute(f"""
                SELECT
                    tc.table_name AS table_name,
                    kcu.column_name AS column_name,
                    ccu.table_name AS foreign_table_name,
                    ccu.column_name AS foreign_column_name
                FROM
                    information_schema.table_constraints AS tc
                JOIN
                    information_schema.key_column_usage AS kcu
                    ON tc.constraint_name = kcu.constraint_name
                JOIN
                    information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                WHERE
                    tc.constraint_type = 'FOREIGN KEY'
                    AND tc.table_schema = '{self.schema}'
                ORDER BY
                    tc.table_name,
                    kcu.column_name
            """)
            
            fk_relations = self.cur.fetchall()
            
            table_relations = {}
            foreign_key_links = []
            
            if fk_relations:
                for relation in fk_relations:
                    source_table = relation['table_name']
                    source_column = relation['column_name']
                    target_table = relation['foreign_table_name']
                    target_column = relation['foreign_column_name']
                    
                    if source_table not in table_relations:
                        table_relations[source_table] = {'references': [], 'referenced_by': []}
                    
                    if target_table not in table_relations:
                        table_relations[target_table] = {'references': [], 'referenced_by': []}
                    
                    # æ·»åŠ å¼•ç”¨å…³ç³»
                    table_relations[source_table]['references'].append({
                        'table': target_table,
                        'source_column': source_column,
                        'target_column': target_column
                    })
                    
                    # æ·»åŠ è¢«å¼•ç”¨å…³ç³»
                    table_relations[target_table]['referenced_by'].append({
                        'table': source_table,
                        'source_column': source_column,
                        'target_column': target_column
                    })
                    
                    # å­˜å‚¨é“¾æ¥
                    foreign_key_links.append({
                        'source_table': source_table,
                        'source_column': source_column,
                        'target_table': target_table,
                        'target_column': target_column
                    })
            
            # æŸ¥æ‰¾æ½œåœ¨çš„éšå¼å…³ç³»ï¼ˆç›¸åŒåˆ—åä½†æ²¡æœ‰å¤–é”®çº¦æŸï¼‰
            implicit_relationships = []
            
            # è·å–æ‰€æœ‰è¡¨
            self.cur.execute(f"""
                SELECT table_name
                FROM information_schema.tables
                WHERE table_schema = '{self.schema}'
                AND table_type = 'BASE TABLE'
            """)
            
            all_tables = [row['table_name'] for row in self.cur.fetchall()]
            
            # å¯¹äºæ¯ä¸ªè¡¨ï¼ŒæŸ¥æ‰¾å…·æœ‰ç›¸åŒåç§°çš„åˆ—
            for i, table1 in enumerate(all_tables):
                for table2 in all_tables[i+1:]:
                    # æŸ¥æ‰¾ä¸¤ä¸ªè¡¨ä¸­å…·æœ‰ç›¸åŒåç§°çš„åˆ—
                    self.cur.execute(f"""
                        SELECT 
                            t1.column_name
                        FROM 
                            information_schema.columns t1
                        JOIN 
                            information_schema.columns t2
                            ON t1.column_name = t2.column_name
                        WHERE 
                            t1.table_schema = '{self.schema}'
                            AND t2.table_schema = '{self.schema}'
                            AND t1.table_name = '{table1}'
                            AND t2.table_name = '{table2}'
                            AND t1.column_name != 'id'  -- æ’é™¤å¸¸è§çš„æ ‡è¯†ç¬¦åˆ—
                            AND t1.column_name NOT LIKE '%_id'  -- æ’é™¤å·²ç»åœ¨å¤–é”®å…³ç³»ä¸­çš„åˆ—
                            AND t1.column_name NOT LIKE 'created_%'  -- æ’é™¤å¸¸è§çš„å…ƒæ•°æ®åˆ—
                            AND t1.column_name NOT LIKE 'updated_%'
                    """)
                    
                    common_columns = [row['column_name'] for row in self.cur.fetchall()]
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å¤–é”®å…³ç³»
                    existing_fk = next((link for link in foreign_key_links if 
                                     (link['source_table'] == table1 and link['target_table'] == table2) or
                                     (link['source_table'] == table2 and link['target_table'] == table1)), None)
                    
                    # å¦‚æœæ²¡æœ‰å¤–é”®å…³ç³»ä½†æœ‰å…±åŒåˆ—ï¼Œæ£€æŸ¥å€¼æ˜¯å¦åŒ¹é…
                    if not existing_fk and common_columns:
                        for column in common_columns:
                            # æ£€æŸ¥ä¸¤ä¸ªè¡¨ä¸­åˆ—æ˜¯å¦åŒ…å«åŒ¹é…å€¼
                            self.cur.execute(f"""
                                SELECT COUNT(*) as match_count
                                FROM (
                                    SELECT DISTINCT {column} as val
                                    FROM {self.schema}.{table1}
                                    WHERE {column} IS NOT NULL
                                    INTERSECT
                                    SELECT DISTINCT {column} as val
                                    FROM {self.schema}.{table2}
                                    WHERE {column} IS NOT NULL
                                ) t
                            """)
                            
                            match_result = self.cur.fetchone()
                            match_count = match_result['match_count'] if match_result else 0
                            
                            if match_count > 0:
                                # æŸ¥çœ‹ä¸¤ä¸ªè¡¨ä¸­åˆ—çš„é‡å ç™¾åˆ†æ¯”
                                self.cur.execute(f"""
                                    WITH table1_values AS (
                                        SELECT DISTINCT {column} as val
                                        FROM {self.schema}.{table1}
                                        WHERE {column} IS NOT NULL
                                    ),
                                    table2_values AS (
                                        SELECT DISTINCT {column} as val
                                        FROM {self.schema}.{table2}
                                        WHERE {column} IS NOT NULL
                                    ),
                                    intersection AS (
                                        SELECT val FROM table1_values
                                        INTERSECT
                                        SELECT val FROM table2_values
                                    )
                                    SELECT
                                        (SELECT COUNT(*) FROM table1_values) as table1_distinct,
                                        (SELECT COUNT(*) FROM table2_values) as table2_distinct,
                                        (SELECT COUNT(*) FROM intersection) as intersection_count
                                """)
                                
                                overlap_result = self.cur.fetchone()
                                
                                if overlap_result:
                                    table1_distinct = overlap_result['table1_distinct']
                                    table2_distinct = overlap_result['table2_distinct']
                                    intersection_count = overlap_result['intersection_count']
                                    
                                    # è®¡ç®—é‡å ç™¾åˆ†æ¯”
                                    if table1_distinct > 0 and table2_distinct > 0:
                                        overlap_pct1 = (intersection_count / table1_distinct) * 100
                                        overlap_pct2 = (intersection_count / table2_distinct) * 100
                                        
                                        # å¦‚æœé‡å è¶³å¤Ÿå¤§ï¼Œè®¤ä¸ºå­˜åœ¨éšå¼å…³ç³»
                                        if overlap_pct1 > 5 or overlap_pct2 > 5:
                                            implicit_relationships.append({
                                                'table1': table1,
                                                'table2': table2,
                                                'column': column,
                                                'table1_distinct_values': table1_distinct,
                                                'table2_distinct_values': table2_distinct,
                                                'matching_values': intersection_count,
                                                'table1_overlap_pct': round(overlap_pct1, 2),
                                                'table2_overlap_pct': round(overlap_pct2, 2)
                                            })
            
            # æ˜¾ç¤ºæ˜¾å¼å…³ç³»
            if foreign_key_links:
                display(Markdown("**æ˜¾å¼å¤–é”®å…³ç³»:**"))
                fk_df = pd.DataFrame(foreign_key_links)
                display(fk_df)
                
                # æ„å»ºæŒ‡å‘æˆ–å¼•ç”¨æœ€å¤šçš„è¡¨æ’å
                references_count = {}
                referenced_by_count = {}
                
                for table, relations in table_relations.items():
                    references_count[table] = len(relations['references'])
                    referenced_by_count[table] = len(relations['referenced_by'])
                
                # æœ€å¸¸è¢«å¼•ç”¨çš„è¡¨
                top_referenced = sorted(referenced_by_count.items(), key=lambda x: x[1], reverse=True)[:5]
                # å¼•ç”¨æœ€å¤šå…¶ä»–è¡¨çš„è¡¨
                top_referencing = sorted(references_count.items(), key=lambda x: x[1], reverse=True)[:5]
                
                display(Markdown("**æœ€å¸¸è¢«å¼•ç”¨çš„è¡¨:**"))
                display(pd.DataFrame(top_referenced, columns=['è¡¨å', 'è¢«å¼•ç”¨æ¬¡æ•°']))
                
                display(Markdown("**å¼•ç”¨æœ€å¤šå…¶ä»–è¡¨çš„è¡¨:**"))
                display(pd.DataFrame(top_referencing, columns=['è¡¨å', 'å¼•ç”¨å…¶ä»–è¡¨æ¬¡æ•°']))
            
            # æ˜¾ç¤ºéšå¼å…³ç³»
            if implicit_relationships:
                display(Markdown("**æ½œåœ¨çš„éšå¼å…³ç³»:**"))
                implicit_df = pd.DataFrame(implicit_relationships)
                display(implicit_df)
                
                # æä¾›åˆ›å»ºå¤–é”®çš„å»ºè®®
                display(Markdown("**æ½œåœ¨å¤–é”®å…³ç³»å»ºè®®:**"))
                
                for relation in implicit_relationships:
                    table1 = relation['table1']
                    table2 = relation['table2']
                    column = relation['column']
                    pct1 = relation['table1_overlap_pct']
                    pct2 = relation['table2_overlap_pct']
                    
                    if pct1 > 50 and pct2 > 50:
                        display(Markdown(f"- **å¼ºå…³ç³»**: `{table1}.{column}` å’Œ `{table2}.{column}` æœ‰æ˜¾è‘—é‡å  ({pct1}% / {pct2}%)ï¼Œåº”è€ƒè™‘å»ºç«‹å¤–é”®çº¦æŸ"))
                    else:
                        display(Markdown(f"- **å¼±å…³ç³»**: `{table1}.{column}` å’Œ `{table2}.{column}` æœ‰éƒ¨åˆ†é‡å  ({pct1}% / {pct2}%)ï¼Œå¯èƒ½å­˜åœ¨å…³ç³»"))
            
            # å­˜å‚¨å…³ç³»åˆ†æç»“æœ
            self.relationship_analysis = {
                'table_relations': table_relations,
                'foreign_key_links': foreign_key_links,
                'implicit_relationships': implicit_relationships
            }
        
        except Exception as e:
            print(f"âŒ åˆ†æè¡¨é—´å…³ç³»æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_entity_connections(self):
        """åˆ†æå®ä½“é—´çš„å…³è”"""
        display(HTML("<h3>å®ä½“å…³è”åˆ†æ</h3>"))
        
        try:
            # å®šä¹‰FDAåŒ»ç–—è®¾å¤‡æ•°æ®åº“ä¸­çš„ä¸»è¦å®ä½“åŠå…¶å…³é”®è¡¨
            entities = {
                'è®¾å¤‡': ['product_codes', 'device_classifications', 'udi_records'],
                'å¬å›': ['device_recalls'],
                'ä¸è‰¯äº‹ä»¶': ['adverse_events', 'event_devices', 'event_patients'],
                'æ‰§æ³•è¡ŒåŠ¨': ['enforcement_actions'],
                'å…¬å¸': ['companies']
            }
            
            # æ¢ç´¢å…³é”®è¡¨ä¹‹é—´çš„ç›´æ¥å…³è”
            display(Markdown("**ä¸»è¦å®ä½“å…³è”:**"))
            
            # äº§å“ä»£ç ä¸ä¸è‰¯äº‹ä»¶çš„å…³è”
            self.cur.execute("""
                SELECT 
                    pc.product_code,
                    pc.device_name,
                    COUNT(DISTINCT ed.event_id) as adverse_event_count
                FROM device.product_codes pc
                JOIN device.event_devices ed ON pc.id = ed.product_code_id
                GROUP BY pc.product_code, pc.device_name
                ORDER BY adverse_event_count DESC
                LIMIT 10
            """)
            
            product_events = self.cur.fetchall()
            
            if product_events:
                display(Markdown("**äº§å“ä¸ä¸è‰¯äº‹ä»¶çš„ä¸»è¦å…³è” (å‰10å):**"))
                display(pd.DataFrame(product_events))
            
            # äº§å“ä»£ç ä¸å¬å›çš„å…³è”
            self.cur.execute("""
                SELECT 
                    pc.product_code,
                    pc.device_name,
                    COUNT(DISTINCT dr.id) as recall_count
                FROM device.product_codes pc
                JOIN device.device_recalls dr ON pc.id = dr.product_code_id
                GROUP BY pc.product_code, pc.device_name
                ORDER BY recall_count DESC
                LIMIT 10
            """)
            
            product_recalls = self.cur.fetchall()
            
            if product_recalls:
                display(Markdown("**äº§å“ä¸å¬å›çš„ä¸»è¦å…³è” (å‰10å):**"))
                display(pd.DataFrame(product_recalls))
            
            # å…¬å¸ä¸äº§å“çš„å…³è”
            self.cur.execute("""
                SELECT 
                    c.name as company_name,
                    COUNT(DISTINCT pc.id) as product_count
                FROM device.companies c
                JOIN device.product_codes pc ON pc.id IN (
                    SELECT product_code_id FROM device.udi_records WHERE company_id = c.id
                )
                GROUP BY c.name
                ORDER BY product_count DESC
                LIMIT 10
            """)
            
            company_products = self.cur.fetchall()
            
            if company_products:
                display(Markdown("**å…¬å¸ä¸äº§å“çš„ä¸»è¦å…³è” (å‰10å):**"))
                display(pd.DataFrame(company_products))
            
            # åˆ†æå…³é”®å®ä½“çš„å…³è”å¼ºåº¦
            display(HTML("<h4>å®ä½“å…³è”å¼ºåº¦åˆ†æ</h4>"))
            
            # ä¼ä¸šä¸ä¸è‰¯äº‹ä»¶å…³è”
            self.cur.execute("""
                SELECT 
                    c.name as company_name,
                    COUNT(DISTINCT ae.id) as adverse_event_count
                FROM device.companies c
                JOIN device.adverse_events ae ON c.id = ae.company_id
                GROUP BY c.name
                ORDER BY adverse_event_count DESC
                LIMIT 10
            """)
            
            company_events = self.cur.fetchall()
            
            if company_events:
                display(Markdown("**ä¼ä¸šä¸ä¸è‰¯äº‹ä»¶çš„ä¸»è¦å…³è” (å‰10å):**"))
                display(pd.DataFrame(company_events))
                
                # å¯è§†åŒ–å‰10å®¶å…¬å¸çš„ä¸è‰¯äº‹ä»¶æ•°é‡
                plt.figure(figsize=(12, 6))
                plt.bar([row['company_name'] for row in company_events], 
                        [row['adverse_event_count'] for row in company_events])
                plt.title('å‰10å®¶å…¬å¸çš„ä¸è‰¯äº‹ä»¶æ•°é‡')
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                plt.show()
            
            # ç”Ÿæˆå®ä½“å…³ç³»æ¦‚è¿°
            display(HTML("<h4>å®ä½“å…³ç³»æ¦‚è¿°</h4>"))
            
            # è®¡ç®—å„å®ä½“è¡¨çš„è®°å½•æ•°
            entity_counts = {}
            for entity, tables in entities.items():
                entity_counts[entity] = 0
                for table in tables:
                    if table in self.table_stats:
                        entity_counts[entity] += self.table_stats[table]['row_count']
            
            # è®¡ç®—å®ä½“é—´çš„å…³è”æ•°
            entity_connections = []
            
            # äº§å“ä¸ä¸è‰¯äº‹ä»¶
            self.cur.execute("""
                SELECT COUNT(DISTINCT ed.event_id) as connection_count
                FROM device.product_codes pc
                JOIN device.event_devices ed ON pc.id = ed.product_code_id
            """)
            
            result = self.cur.fetchone()
            if result and result['connection_count']:
                entity_connections.append({
                    'æºå®ä½“': 'è®¾å¤‡',
                    'ç›®æ ‡å®ä½“': 'ä¸è‰¯äº‹ä»¶',
                    'å…³è”æ•°é‡': result['connection_count'],
                    'å…³è”ç±»å‹': 'äº§å“ä»£ç å…³è”'
                })
            
            # äº§å“ä¸å¬å›
            self.cur.execute("""
                SELECT COUNT(DISTINCT dr.id) as connection_count
                FROM device.product_codes pc
                JOIN device.device_recalls dr ON pc.id = dr.product_code_id
            """)
            
            result = self.cur.fetchone()
            if result and result['connection_count']:
                entity_connections.append({
                    'æºå®ä½“': 'è®¾å¤‡',
                    'ç›®æ ‡å®ä½“': 'å¬å›',
                    'å…³è”æ•°é‡': result['connection_count'],
                    'å…³è”ç±»å‹': 'äº§å“ä»£ç å…³è”'
                })
            
            # å…¬å¸ä¸äº§å“
            self.cur.execute("""
                SELECT COUNT(DISTINCT ur.id) as connection_count
                FROM device.companies c
                JOIN device.udi_records ur ON c.id = ur.company_id
            """)
            
            result = self.cur.fetchone()
            if result and result['connection_count']:
                entity_connections.append({
                    'æºå®ä½“': 'å…¬å¸',
                    'ç›®æ ‡å®ä½“': 'è®¾å¤‡',
                    'å…³è”æ•°é‡': result['connection_count'],
                    'å…³è”ç±»å‹': 'UDIè®°å½•å…³è”'
                })
            
            # å…¬å¸ä¸ä¸è‰¯äº‹ä»¶
            self.cur.execute("""
                SELECT COUNT(DISTINCT ae.id) as connection_count
                FROM device.companies c
                JOIN device.adverse_events ae ON c.id = ae.company_id
            """)
            
            result = self.cur.fetchone()
            if result and result['connection_count']:
                entity_connections.append({
                    'æºå®ä½“': 'å…¬å¸',
                    'ç›®æ ‡å®ä½“': 'ä¸è‰¯äº‹ä»¶',
                    'å…³è”æ•°é‡': result['connection_count'],
                    'å…³è”ç±»å‹': 'å…¬å¸IDå…³è”'
                })
            
            # å…¬å¸ä¸å¬å›
            self.cur.execute("""
                SELECT COUNT(DISTINCT dr.id) as connection_count
                FROM device.companies c
                JOIN device.device_recalls dr ON c.id = dr.company_id
            """)
            
            result = self.cur.fetchone()
            if result and result['connection_count']:
                entity_connections.append({
                    'æºå®ä½“': 'å…¬å¸',
                    'ç›®æ ‡å®ä½“': 'å¬å›',
                    'å…³è”æ•°é‡': result['connection_count'],
                    'å…³è”ç±»å‹': 'å…¬å¸IDå…³è”'
                })
            
            # æ˜¾ç¤ºå®ä½“å…³ç³»æ¦‚è¿°
            display(Markdown("**å®ä½“æ•°é‡:**"))
            display(pd.DataFrame(list(entity_counts.items()), columns=['å®ä½“', 'è®°å½•æ•°']))
            
            display(Markdown("**å®ä½“é—´å…³è”:**"))
            display(pd.DataFrame(entity_connections))
            
            # æä¾›å®ä½“è¿æ¥è´¨é‡åˆ†æ
            display(HTML("<h4>å®ä½“è¿æ¥è´¨é‡åˆ†æ</h4>"))
            
            # åˆ†æè¿æ¥å®Œæ•´æ€§
            connection_quality = []
            
            # äº§å“ä»£ç è¿æ¥å®Œæ•´æ€§
            self.cur.execute("""
                SELECT 
                    'event_devices' as table_name,
                    COUNT(*) as total_records,
                    SUM(CASE WHEN product_code_id IS NULL THEN 1 ELSE 0 END) as missing_connections,
                    ROUND((SUM(CASE WHEN product_code_id IS NULL THEN 1 ELSE 0 END)::float / COUNT(*)) * 100, 2) as missing_percentage
                FROM device.event_devices
            """)
            
            result = self.cur.fetchone()
            if result:
                connection_quality.append({
                    'è¡¨å': result['table_name'],
                    'æ€»è®°å½•æ•°': result['total_records'],
                    'ç¼ºå¤±è¿æ¥æ•°': result['missing_connections'],
                    'ç¼ºå¤±ç™¾åˆ†æ¯”': result['missing_percentage']
                })
            
            # å…¬å¸è¿æ¥å®Œæ•´æ€§ï¼ˆä¸è‰¯äº‹ä»¶ï¼‰
            self.cur.execute("""
                SELECT 
                    'adverse_events' as table_name,
                    COUNT(*) as total_records,
                    SUM(CASE WHEN company_id IS NULL THEN 1 ELSE 0 END) as missing_connections,
                    ROUND((SUM(CASE WHEN company_id IS NULL THEN 1 ELSE 0 END)::float / COUNT(*)) * 100, 2) as missing_percentage
                FROM device.adverse_events
            """)
            
            result = self.cur.fetchone()
            if result:
                connection_quality.append({
                    'è¡¨å': result['table_name'],
                    'æ€»è®°å½•æ•°': result['total_records'],
                    'ç¼ºå¤±è¿æ¥æ•°': result['missing_connections'],
                    'ç¼ºå¤±ç™¾åˆ†æ¯”': result['missing_percentage']
                })
            
            # å…¬å¸è¿æ¥å®Œæ•´æ€§ï¼ˆå¬å›ï¼‰
            self.cur.execute("""
                SELECT 
                    'device_recalls' as table_name,
                    COUNT(*) as total_records,
                    SUM(CASE WHEN company_id IS NULL THEN 1 ELSE 0 END) as missing_connections,
                    ROUND((SUM(CASE WHEN company_id IS NULL THEN 1 ELSE 0 END)::float / COUNT(*)) * 100, 2) as missing_percentage
                FROM device.device_recalls
            """)
            
            result = self.cur.fetchone()
            if result:
                connection_quality.append({
                    'è¡¨å': result['table_name'],
                    'æ€»è®°å½•æ•°': result['total_records'],
                    'ç¼ºå¤±è¿æ¥æ•°': result['missing_connections'],
                    'ç¼ºå¤±ç™¾åˆ†æ¯”': result['missing_percentage']
                })
            
            # æ˜¾ç¤ºè¿æ¥è´¨é‡åˆ†æ
            if connection_quality:
                display(Markdown("**å®ä½“è¿æ¥è´¨é‡:**"))
                display(pd.DataFrame(connection_quality))
                
                # æä¾›å»ºè®®
                poor_connections = [row for row in connection_quality if row['ç¼ºå¤±ç™¾åˆ†æ¯”'] > 10]
                if poor_connections:
                    display(Markdown("**è¿æ¥è´¨é‡é—®é¢˜:**"))
                    for conn in poor_connections:
                        display(Markdown(f"- {conn['è¡¨å']} ä¸­æœ‰ {conn['ç¼ºå¤±ç™¾åˆ†æ¯”']}% çš„è®°å½•ç¼ºå°‘å…³è”ï¼Œè¿™å¯èƒ½å½±å“è·¨å®ä½“åˆ†æçš„å‡†ç¡®æ€§"))
                    
                    display(Markdown("""
                    **æé«˜å®ä½“è¿æ¥è´¨é‡çš„å»ºè®®:**
                    
                    1. **å®Œå–„äº§å“ä»£ç æ˜ å°„:**
                       - ä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…æˆ–æ¨¡ç³ŠåŒ¹é…æŠ€æœ¯æ˜ å°„ç¼ºå¤±çš„äº§å“ä»£ç 
                       - è€ƒè™‘ä½¿ç”¨è¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚äº§å“åç§°ï¼‰è¾…åŠ©åŒ¹é…
                       
                    2. **å®Œå–„å…¬å¸æ˜ å°„:**
                       - ä½¿ç”¨å…¬å¸åç§°æ ‡å‡†åŒ–å’ŒåŒ¹é…æŠ€æœ¯è¯†åˆ«ç›¸åŒå…¬å¸çš„ä¸åŒè¡¨è¿°
                       - è€ƒè™‘ä½¿ç”¨å¤–éƒ¨å…¬å¸æ•°æ®æºï¼ˆå¦‚DUNSç­‰ï¼‰æ¥å®Œå–„æ˜ å°„
                       
                    3. **æ”¹è¿›æ•°æ®é‡‡é›†æµç¨‹:**
                       - å¦‚æœæ‚¨è´Ÿè´£æ•°æ®é‡‡é›†ï¼Œç¡®ä¿æ”¶é›†å…³é”®çš„è¿æ¥å­—æ®µ
                       - è€ƒè™‘ä½¿ç”¨è·¨å¼•ç”¨éªŒè¯ç¡®ä¿å…³è”å®Œæ•´æ€§
                    """))
        
        except Exception as e:
            print(f"âŒ åˆ†æå®ä½“å…³è”æ—¶å‡ºé”™: {str(e)}")
    
    def generate_optimization_recommendations(self):
        """ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–å»ºè®®"""
        display(HTML("<h3>æ•°æ®åº“ä¼˜åŒ–å»ºè®®</h3>"))
        
        try:
            recommendations = []
            
            # 1. ç´¢å¼•ä¼˜åŒ–å»ºè®®
            display(HTML("<h4>ç´¢å¼•ä¼˜åŒ–å»ºè®®</h4>"))
            
            # 1.1 ç¼ºå°‘ç´¢å¼•çš„å¤§è¡¨
            large_tables_without_indexes = []
            
            for table_name, stats in self.table_stats.items():
                if stats['row_count'] > 100000:  # å¤§äº10ä¸‡è¡Œçš„è¡¨
                    if table_name not in self.index_analysis or self.index_analysis[table_name]['total_indexes'] < 3:
                        large_tables_without_indexes.append({
                            'è¡¨å': table_name,
                            'è¡Œæ•°': stats['row_count'],
                            'ç´¢å¼•æ•°': self.index_analysis.get(table_name, {}).get('total_indexes', 0)
                        })
            
            if large_tables_without_indexes:
                display(Markdown("**å¤§è¡¨ç¼ºå°‘è¶³å¤Ÿç´¢å¼•:**"))
                display(pd.DataFrame(large_tables_without_indexes))
                
                # ä¸ºè¿™äº›è¡¨ç”Ÿæˆå»ºè®®çš„ç´¢å¼•
                display(Markdown("**å»ºè®®æ·»åŠ ä»¥ä¸‹ç´¢å¼•:**"))
                
                for table_info in large_tables_without_indexes:
                    table_name = table_info['è¡¨å']
                    
                    # æŸ¥æ‰¾è¯¥è¡¨çš„å¸¸ç”¨æŸ¥è¯¢åˆ—
                    self.cur.execute(f"""
                        SELECT column_name, data_type
                        FROM information_schema.columns
                        WHERE table_schema = '{self.schema}'
                        AND table_name = '{table_name}'
                        AND (
                            column_name LIKE '%\\_id' OR
                            column_name LIKE '%date%' OR
                            column_name LIKE '%code%' OR
                            column_name LIKE '%number%' OR
                            column_name LIKE '%key%' OR
                            column_name LIKE '%status%'
                        )
                        AND column_name NOT IN (
                            SELECT a.attname
                            FROM pg_class t
                            JOIN pg_index ix ON t.oid = ix.indrelid
                            JOIN pg_class i ON i.oid = ix.indexrelid
                            JOIN pg_attribute a ON a.attrelid = t.oid AND a.attnum = ANY(ix.indkey)
                            WHERE t.relname = '{table_name}'
                            AND t.relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = '{self.schema}')
                        )
                        LIMIT 5
                    """)
                    
                    potential_index_columns = self.cur.fetchall()
                    
                    for i, col in enumerate(potential_index_columns):
                        column_name = col['column_name']
                        data_type = col['data_type']
                        
                        # ä¸ºè¿™äº›åˆ—ç”ŸæˆCREATE INDEXè¯­å¥
                        index_name = f"idx_{table_name}_{column_name}"
                        
                        sql = f"CREATE INDEX {index_name} ON {self.schema}.{table_name} ({column_name});"
                        display(Markdown(f"```sql\n{sql}\n```"))
                        
                        # æä¾›è§£é‡Š
                        if "date" in column_name:
                            display(Markdown(f"- ä¸º `{table_name}.{column_name}` æ·»åŠ ç´¢å¼•å°†åŠ é€ŸæŒ‰æ—¥æœŸèŒƒå›´çš„æŸ¥è¯¢"))
                        elif "id" in column_name:
                            display(Markdown(f"- ä¸º `{table_name}.{column_name}` æ·»åŠ ç´¢å¼•å°†åŠ é€Ÿä¸å…¶ä»–è¡¨çš„è¿æ¥æŸ¥è¯¢"))
                        elif "code" in column_name or "number" in column_name:
                            display(Markdown(f"- ä¸º `{table_name}.{column_name}` æ·»åŠ ç´¢å¼•å°†åŠ é€ŸæŒ‰ä»£ç æˆ–ç¼–å·çš„è¿‡æ»¤æŸ¥è¯¢"))
                        elif "status" in column_name:
                            display(Markdown(f"- ä¸º `{table_name}.{column_name}` æ·»åŠ ç´¢å¼•å°†åŠ é€ŸæŒ‰çŠ¶æ€è¿‡æ»¤çš„æŸ¥è¯¢"))
            
            # 1.2 æ£€æŸ¥ç´¢å¼•å†—ä½™
            redundant_indexes = []
            
            for table_name, index_info in self.index_analysis.items():
                if index_info['total_indexes'] > 5:  # ç´¢å¼•æ•°é‡è¾ƒå¤šçš„è¡¨
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„ç´¢å¼•åˆ—
                    similar_indexes = []
                    
                    for i, index1 in enumerate(index_info.get('index_details', {}).items()):
                        index1_name, index1_data = index1
                        index1_columns = index1_data['columns'].split(', ')
                        
                        for j, index2 in enumerate(list(index_info.get('index_details', {}).items())[i+1:]):
                            index2_name, index2_data = index2
                            index2_columns = index2_data['columns'].split(', ')
                            
                            # æ£€æŸ¥ä¸¤ä¸ªç´¢å¼•æ˜¯å¦è¦†ç›–ç›¸ä¼¼çš„åˆ—
                            if len(index1_columns) > 0 and len(index2_columns) > 0:
                                if index1_columns[0] in index2_columns:
                                    similar_indexes.append({
                                        'è¡¨å': table_name,
                                        'ç´¢å¼•1': index1_name,
                                        'åˆ—1': index1_data['columns'],
                                        'ç´¢å¼•2': index2_name,
                                        'åˆ—2': index2_data['columns']
                                    })
                    
                    if similar_indexes:
                        redundant_indexes.extend(similar_indexes)
            
            if redundant_indexes:
                display(Markdown("**æ½œåœ¨å†—ä½™ç´¢å¼•:**"))
                display(pd.DataFrame(redundant_indexes))
                
                display(Markdown("""
                **å¤„ç†å†—ä½™ç´¢å¼•çš„å»ºè®®:**
                
                1. å®¡æŸ¥è¿™äº›ç›¸ä¼¼ç´¢å¼•ï¼Œç¡®å®šå“ªäº›å¯ä»¥åˆå¹¶æˆ–åˆ é™¤
                2. ä¼˜å…ˆä¿ç•™å¤šåˆ—å¤åˆç´¢å¼•ï¼Œå®ƒä»¬é€šå¸¸å¯ä»¥æ›¿ä»£å•åˆ—ç´¢å¼•
                3. ä½¿ç”¨æ•°æ®åº“å·¥å…·(å¦‚pg_stat_statements)ç›‘æ§ç´¢å¼•ä½¿ç”¨æƒ…å†µï¼Œåˆ é™¤æœªä½¿ç”¨çš„ç´¢å¼•
                
                ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä»¥ä¸‹ä¸¤ä¸ªç´¢å¼•ï¼š
                - idx_table_col1
                - idx_table_col1_col2
                
                é€šå¸¸å¯ä»¥åªä¿ç•™ç¬¬äºŒä¸ªå¤åˆç´¢å¼•(idx_table_col1_col2)ï¼Œå®ƒåŒæ—¶æ”¯æŒå¯¹col1å’Œ(col1,col2)çš„æŸ¥è¯¢ã€‚
                """))
            
            # 2. è¡¨åˆ†åŒºå»ºè®®
            display(HTML("<h4>è¡¨åˆ†åŒºå»ºè®®</h4>"))
            
            # ç¡®å®šå¯èƒ½é€‚åˆåˆ†åŒºçš„å¤§è¡¨
            partition_candidates = []
            
            for table_name, stats in self.table_stats.items():
                if stats['row_count'] > 10000000:  # è¶…è¿‡1000ä¸‡è¡Œçš„è¡¨
                    # æ£€æŸ¥æ˜¯å¦æœ‰é€‚åˆåˆ†åŒºçš„æ—¥æœŸåˆ—
                    self.cur.execute(f"""
                        SELECT column_name, data_type
                        FROM information_schema.columns
                        WHERE table_schema = '{self.schema}'
                        AND table_name = '{table_name}'
                        AND (data_type LIKE '%date%' OR data_type LIKE '%time%')
                    """)
                    
                    date_columns = self.cur.fetchall()
                    
                    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªé€‚åˆåˆ†åŒºçš„æ—¥æœŸåˆ—
                    partition_column = None
                    date_range = None
                    
                    for col in date_columns:
                        column_name = col['column_name']
                        
                        self.cur.execute(f"""
                            SELECT 
                                MIN({column_name}) as min_date,
                                MAX({column_name}) as max_date,
                                COUNT(DISTINCT EXTRACT(YEAR FROM {column_name})) as num_years
                            FROM {self.schema}.{table_name}
                            WHERE {column_name} IS NOT NULL
                        """)
                        
                        result = self.cur.fetchone()
                        
                        if result and result['min_date'] and result['max_date'] and result['num_years'] > 1:
                            partition_column = column_name
                            date_range = {
                                'min_date': result['min_date'],
                                'max_date': result['max_date'],
                                'num_years': result['num_years']
                            }
                            break
                    
                    if partition_column:
                        partition_candidates.append({
                            'è¡¨å': table_name,
                            'è¡Œæ•°': stats['row_count'],
                            'åˆ†åŒºåˆ—': partition_column,
                            'æ—¥æœŸèŒƒå›´': f"{date_range['min_date']} è‡³ {date_range['max_date']}",
                            'å¹´æ•°': date_range['num_years']
                        })
            
            if partition_candidates:
                display(Markdown("**é€‚åˆåˆ†åŒºçš„å¤§è¡¨:**"))
                display(pd.DataFrame(partition_candidates))
                
                display(Markdown("""
                **è¡¨åˆ†åŒºå»ºè®®:**
                
                å¤§å‹è¡¨çš„åˆ†åŒºå¯ä»¥æ˜¾è‘—æé«˜æŸ¥è¯¢æ€§èƒ½å’Œç®¡ç†æ•ˆç‡ï¼Œå°¤å…¶æ˜¯æ—¶é—´åºåˆ—æ•°æ®ã€‚PostgreSQLæ”¯æŒå£°æ˜å¼åˆ†åŒºï¼Œæ¨èæŒ‰å¹´æˆ–æŒ‰æœˆåˆ†åŒºï¼š
                
                ```sql
                -- ç¤ºä¾‹ï¼šæŒ‰æœˆåˆ†åŒºè¡¨
                CREATE TABLE partitioned_table (
                    id SERIAL,
                    date_column DATE,
                    -- å…¶ä»–åˆ—
                ) PARTITION BY RANGE (date_column);
                
                -- åˆ›å»ºæœˆåº¦åˆ†åŒº
                CREATE TABLE partitioned_table_y2022m01 PARTITION OF partitioned_table
                    FOR VALUES FROM ('2022-01-01') TO ('2022-02-01');
                CREATE TABLE partitioned_table_y2022m02 PARTITION OF partitioned_table
                    FOR VALUES FROM ('2022-02-01') TO ('2022-03-01');
                -- æ›´å¤šåˆ†åŒº...
                ```
                
                **åˆ†åŒºå®æ–½å»ºè®®:**
                
                1. å¯¹äºå†å²æ•°æ®ï¼Œåœ¨ç»´æŠ¤çª—å£æœŸé—´æ‰§è¡Œåˆ†åŒºè¿ç§»
                2. ä½¿ç”¨ä¸´æ—¶è¡¨+é‡å‘½åç­–ç•¥æœ€å°åŒ–åœæœºæ—¶é—´
                3. ä¸ºæ–°çš„åˆ†åŒºåˆ›å»ºç›¸åŒçš„ç´¢å¼•ç»“æ„
                4. è€ƒè™‘è‡ªåŠ¨åŒ–åˆ†åŒºåˆ›å»ºä»¥å¤„ç†æ–°æ•°æ®
                """))
            
            # 3. æ•°æ®å½’æ¡£å»ºè®®
            display(HTML("<h4>æ•°æ®å½’æ¡£å»ºè®®</h4>"))
            
            # æ£€æŸ¥åŒ…å«æ—§æ•°æ®çš„è¡¨
            archive_candidates = []
            
            for table_name, stats in self.table_stats.items():
                if stats['row_count'] > 1000000:  # è¶…è¿‡100ä¸‡è¡Œçš„è¡¨
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸåˆ—
                    self.cur.execute(f"""
                        SELECT column_name
                        FROM information_schema.columns
                        WHERE table_schema = '{self.schema}'
                        AND table_name = '{table_name}'
                        AND (data_type LIKE '%date%' OR data_type LIKE '%time%')
                    """)
                    
                    date_columns = [row['column_name'] for row in self.cur.fetchall()]
                    
                    # å¦‚æœæœ‰æ—¥æœŸåˆ—ï¼Œæ£€æŸ¥æœ‰å¤šå°‘æ—§æ•°æ®
                    if date_columns:
                        for date_column in date_columns[:1]:  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªæ—¥æœŸåˆ—
                            self.cur.execute(f"""
                                SELECT
                                    COUNT(*) as total_count,
                                    SUM(CASE WHEN {date_column} < CURRENT_DATE - INTERVAL '5 years' THEN 1 ELSE 0 END) as old_data_count
                                FROM {self.schema}.{table_name}
                                WHERE {date_column} IS NOT NULL
                            """)
                            
                            result = self.cur.fetchone()
                            
                            if result and result['old_data_count'] > 100000:  # è¶…è¿‡10ä¸‡è¡Œæ—§æ•°æ®
                                old_percentage = (result['old_data_count'] / result['total_count']) * 100 if result['total_count'] > 0 else 0
                                
                                if old_percentage > 20:  # è¶…è¿‡20%æ˜¯æ—§æ•°æ®
                                    archive_candidates.append({
                                        'è¡¨å': table_name,
                                        'æ€»è¡Œæ•°': stats['row_count'],
                                        'æ—§æ•°æ®è¡Œæ•°': result['old_data_count'],
                                        'æ—§æ•°æ®ç™¾åˆ†æ¯”': f"{round(old_percentage, 2)}%",
                                        'æ—¥æœŸåˆ—': date_column
                                    })
            
            if archive_candidates:
                display(Markdown("**é€‚åˆæ•°æ®å½’æ¡£çš„è¡¨:**"))
                display(pd.DataFrame(archive_candidates))
                
                display(Markdown("""
                **æ•°æ®å½’æ¡£å»ºè®®:**
                
                å¯¹äºåŒ…å«å¤§é‡å†å²æ•°æ®çš„è¡¨ï¼Œå»ºè®®å®æ–½æ•°æ®å½’æ¡£ç­–ç•¥ä»¥æé«˜æ€§èƒ½å¹¶ä¼˜åŒ–å­˜å‚¨ï¼š
                
                1. **å½’æ¡£ç­–ç•¥ï¼š**
                   - åˆ›å»ºä¸æºè¡¨ç»“æ„ç›¸åŒçš„å½’æ¡£è¡¨
                   - å°†æ—§æ•°æ®ç§»åŠ¨åˆ°å½’æ¡£è¡¨
                   - å»ºç«‹å¯è®¿é—®å½’æ¡£æ•°æ®çš„è§†å›¾
                
                2. **å½’æ¡£æ–¹æ¡ˆç¤ºä¾‹ï¼š**
                   ```sql
                   -- åˆ›å»ºå½’æ¡£è¡¨
                   CREATE TABLE archive_table (LIKE source_table INCLUDING ALL);
                   
                   -- è¿ç§»æ—§æ•°æ®
                   BEGIN;
                   INSERT INTO archive_table SELECT * FROM source_table WHERE date_column < CURRENT_DATE - INTERVAL '5 years';
                   DELETE FROM source_table WHERE date_column < CURRENT_DATE - INTERVAL '5 years';
                   COMMIT;
                   
                   -- åˆ›å»ºç»Ÿä¸€è®¿é—®è§†å›¾
                   CREATE VIEW full_data_view AS
                       SELECT * FROM source_table
                       UNION ALL
                       SELECT * FROM archive_table;
                   ```
                
                3. **å¢é‡å½’æ¡£ï¼š**
                   - è®¾ç½®å®šæœŸå½’æ¡£ä½œä¸šï¼Œä¾‹å¦‚æ¯æœˆæˆ–æ¯å­£åº¦è¿è¡Œä¸€æ¬¡
                   - ä½¿ç”¨æ‰¹å¤„ç†æ–¹æ³•é¿å…é•¿æ—¶é—´é”å®š
                   - è€ƒè™‘åœ¨ä½å³°æ—¶æ®µæ‰§è¡Œå½’æ¡£
                
                4. **å½’æ¡£åæ“ä½œï¼š**
                   - å¯¹å½’æ¡£è¡¨æ‰§è¡ŒVACUUM FULLé‡Šæ”¾ç©ºé—´
                   - é‡å»ºæºè¡¨ç´¢å¼•ä»¥ä¼˜åŒ–æ€§èƒ½
                   - æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯ä»¥ç¡®ä¿æŸ¥è¯¢ä¼˜åŒ–å™¨ä½¿ç”¨æ­£ç¡®çš„è®¡åˆ’
                """))
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–å»ºè®®æ—¶å‡ºé”™: {str(e)}")
    
    def generate_query_recommendations(self):
        """ç”ŸæˆæŸ¥è¯¢ä¼˜åŒ–å»ºè®®"""
        display(HTML("<h3>æŸ¥è¯¢ä¼˜åŒ–å»ºè®®</h3>"))
        
        try:
            # ç”Ÿæˆå¸¸è§æŸ¥è¯¢æ¨¡å¼åŠå…¶ä¼˜åŒ–å»ºè®®
            display(Markdown("""
            åŸºäºå¯¹FDAåŒ»ç–—è®¾å¤‡æ•°æ®åº“ç»“æ„å’Œæ•°æ®ç‰¹ç‚¹çš„åˆ†æï¼Œä»¥ä¸‹æ˜¯å¸¸è§æŸ¥è¯¢åœºæ™¯çš„ä¼˜åŒ–å»ºè®®ï¼š
            
            ### 1. æŸ¥è¯¢ä¸è‰¯äº‹ä»¶æ•°æ®
            
            **å¸¸è§æ¨¡å¼:** æŒ‰æ—¥æœŸèŒƒå›´å’Œäº‹ä»¶ç±»å‹æŸ¥è¯¢ä¸è‰¯äº‹ä»¶
            
            ```sql
            -- æœªä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT *
            FROM device.adverse_events
            WHERE date_received BETWEEN '2020-01-01' AND '2020-12-31'
            AND event_type = 'Malfunction';
            ```
            
            **ä¼˜åŒ–å»ºè®®:**
            ```sql
            -- ä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT ae.id, ae.report_number, ae.date_received, ae.event_type
            FROM device.adverse_events ae
            WHERE ae.date_received BETWEEN '2020-01-01' AND '2020-12-31'
            AND ae.event_type = 'Malfunction'
            LIMIT 1000;  -- é™åˆ¶ç»“æœé›†å¤§å°
            ```
            
            **æ€§èƒ½æå‡ç‚¹:**
            - é€‰æ‹©å¿…è¦çš„åˆ—è€Œéä½¿ç”¨ * 
            - å¯¹å¤§ç»“æœé›†ä½¿ç”¨LIMIT
            - ç¡®ä¿date_receivedå’Œevent_typeåˆ—æœ‰ç´¢å¼•
            
            ### 2. è·¨è¡¨è¿æ¥æŸ¥è¯¢
            
            **å¸¸è§æ¨¡å¼:** å…³è”è®¾å¤‡ã€å…¬å¸å’Œä¸è‰¯äº‹ä»¶æ•°æ®
            
            ```sql
            -- æœªä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT *
            FROM device.adverse_events ae
            JOIN device.event_devices ed ON ae.id = ed.event_id
            JOIN device.product_codes pc ON ed.product_code_id = pc.id
            JOIN device.companies c ON ae.company_id = c.id
            WHERE ae.date_received > '2019-01-01';
            ```
            
            **ä¼˜åŒ–å»ºè®®:**
            ```sql
            -- ä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT 
                ae.report_number,
                ae.date_received,
                ae.event_type,
                pc.product_code,
                pc.device_name,
                c.name as company_name
            FROM device.adverse_events ae
            JOIN device.event_devices ed ON ae.id = ed.event_id
            JOIN device.product_codes pc ON ed.product_code_id = pc.id
            JOIN device.companies c ON ae.company_id = c.id
            WHERE ae.date_received > '2019-01-01'
            ORDER BY ae.date_received DESC
            LIMIT 500;
            ```
            
            **æ€§èƒ½æå‡ç‚¹:**
            - é€‰æ‹©å¿…è¦çš„åˆ—è€Œéä½¿ç”¨ *
            - ç¡®ä¿æ‰€æœ‰è¿æ¥åˆ—éƒ½æœ‰ç´¢å¼•
            - ä¼˜åŒ–WHEREå­å¥ä¸­çš„ç­›é€‰æ¡ä»¶
            - å¯¹ç»“æœè¿›è¡Œæ’åºå’Œåˆ†é¡µ
            
            ### 3. èšåˆæŸ¥è¯¢
            
            **å¸¸è§æ¨¡å¼:** æŒ‰äº§å“ä»£ç å’Œè®¾å¤‡ç±»å‹ç»Ÿè®¡ä¸è‰¯äº‹ä»¶
            
            ```sql
            -- æœªä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT 
                pc.product_code,
                pc.device_name,
                COUNT(*) as event_count
            FROM device.adverse_events ae
            JOIN device.event_devices ed ON ae.id = ed.event_id
            JOIN device.product_codes pc ON ed.product_code_id = pc.id
            GROUP BY pc.product_code, pc.device_name;
            ```
            
            **ä¼˜åŒ–å»ºè®®:**
            ```sql
            -- ä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT 
                pc.product_code,
                pc.device_name,
                COUNT(*) as event_count
            FROM device.adverse_events ae
            JOIN device.event_devices ed ON ae.id = ed.event_id
            JOIN device.product_codes pc ON ed.product_code_id = pc.id
            WHERE ae.date_received > CURRENT_DATE - INTERVAL '5 years'
            GROUP BY pc.product_code, pc.device_name
            HAVING COUNT(*) > 10
            ORDER BY event_count DESC
            LIMIT 100;
            ```
            
            **æ€§èƒ½æå‡ç‚¹:**
            - æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤å™¨å‡å°‘å¤„ç†è®°å½•æ•°
            - ä½¿ç”¨HAVINGå­å¥è¿‡æ»¤ä½ä»·å€¼ç»“æœ
            - å¯¹ç»“æœæ’åºå’Œåˆ†é¡µ
            - è€ƒè™‘åˆ›å»ºç‰©åŒ–è§†å›¾ä»¥åŠ é€Ÿå¸¸ç”¨çš„èšåˆæŸ¥è¯¢
            
            ### 4. æ—¶é—´åºåˆ—åˆ†ææŸ¥è¯¢
            
            **å¸¸è§æ¨¡å¼:** æŒ‰æœˆåˆ†æä¸è‰¯äº‹ä»¶è¶‹åŠ¿
            
            ```sql
            -- æœªä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT 
                DATE_TRUNC('month', date_received) as month,
                COUNT(*) as event_count
            FROM device.adverse_events
            GROUP BY month
            ORDER BY month;
            ```
            
            **ä¼˜åŒ–å»ºè®®:**
            ```sql
            -- ä¼˜åŒ–çš„æŸ¥è¯¢
            SELECT 
                DATE_TRUNC('month', date_received) as month,
                event_type,
                COUNT(*) as event_count
            FROM device.adverse_events
            WHERE date_received >= CURRENT_DATE - INTERVAL '3 years'
            GROUP BY month, event_type
            ORDER BY month DESC, event_count DESC;
            ```
            
            **æ€§èƒ½æå‡ç‚¹:**
            - é™åˆ¶æ—¶é—´èŒƒå›´
            - æŒ‰æ›´å¤šç»´åº¦èšåˆä»¥è·å¾—æ›´è¯¦ç»†çš„æ´å¯Ÿ
            - è€ƒè™‘é¢„è®¡ç®—å¸¸ç”¨æ—¶é—´æ®µçš„èšåˆç»“æœ
            
            ### 5. ä¼˜åŒ–æ•°æ®å¯¼å‡ºæŸ¥è¯¢
            
            **å¸¸è§æ¨¡å¼:** å¯¼å‡ºå¤§é‡æ•°æ®ç”¨äºå¤–éƒ¨åˆ†æ
            
            ```sql
            -- æœªä¼˜åŒ–çš„æŸ¥è¯¢
            COPY (SELECT * FROM device.adverse_events) TO '/tmp/export.csv' WITH CSV HEADER;
            ```
            
            **ä¼˜åŒ–å»ºè®®:**
            ```sql
            -- ä¼˜åŒ–çš„æ•°æ®å¯¼å‡º
            COPY (
                SELECT 
                    ae.report_number,
                    ae.date_received,
                    ae.event_type,
                    pc.product_code,
                    pc.device_name,
                    c.name as company_name
                FROM device.adverse_events ae
                LEFT JOIN device.event_devices ed ON ae.id = ed.event_id
                LEFT JOIN device.product_codes pc ON ed.product_code_id = pc.id
                LEFT JOIN device.companies c ON ae.company_id = c.id
                WHERE ae.date_received > '2020-01-01'
                ORDER BY ae.date_received DESC
                LIMIT 1000000
            ) TO '/tmp/adverse_events_export.csv' WITH (FORMAT CSV, HEADER);
            ```
            
            **æ€§èƒ½æå‡ç‚¹:**
            - åªå¯¼å‡ºå¿…è¦çš„åˆ—å’Œè®°å½•
            - ä½¿ç”¨LEFT JOINé¿å…æ’é™¤æ²¡æœ‰å…³è”è®°å½•çš„è¡Œ
            - è€ƒè™‘åˆ†æ‰¹å¯¼å‡ºå¤§æ•°æ®é›†
            - ä¼˜å…ˆä½¿ç”¨WHEREè¿‡æ»¤è€Œéå®¢æˆ·ç«¯è¿‡æ»¤
            """))
            
            # ä¿å­˜æŸ¥è¯¢å»ºè®®ä»¥ä¾›åç»­åˆ†æ
            self.query_recommendations = {
                'adverse_events': [
                    "ç¡®ä¿date_received, event_type, report_numberåˆ—æœ‰ç´¢å¼•",
                    "ä½¿ç”¨LIMITå’Œåˆ†é¡µå¤„ç†å¤§ç»“æœé›†",
                    "å»ºè®®å¯¹date_receivedåˆ—åˆ›å»ºBRINç´¢å¼•ä»¥æ”¯æŒé«˜æ•ˆçš„æ—¥æœŸèŒƒå›´æ‰«æ"
                ],
                'event_devices': [
                    "ç¡®ä¿event_idå’Œproduct_code_idåˆ—éƒ½æœ‰ç´¢å¼•",
                    "è€ƒè™‘åˆ›å»º(event_id, product_code_id)å¤åˆç´¢å¼•ä»¥ä¼˜åŒ–è¿æ¥æŸ¥è¯¢"
                ],
                'product_codes': [
                    "å¯¹product_codeåˆ—åˆ›å»ºå”¯ä¸€ç´¢å¼•",
                    "ç¡®ä¿device_classåˆ—æœ‰ç´¢å¼•ä»¥æ”¯æŒè¿‡æ»¤æŸ¥è¯¢"
                ],
                'companies': [
                    "ä¸ºnameåˆ—åˆ›å»ºç´¢å¼•ä»¥æ”¯æŒå…¬å¸åç§°æŸ¥è¯¢",
                    "è€ƒè™‘ä¸ºnameåˆ—æ·»åŠ trigramç´¢å¼•ä»¥æ”¯æŒæ¨¡ç³ŠåŒ¹é…"
                ]
            }
        
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŸ¥è¯¢ä¼˜åŒ–å»ºè®®æ—¶å‡ºé”™: {str(e)}")
    
    def generate_preprocessing_recommendations(self):
        """ç”Ÿæˆæ•°æ®é¢„å¤„ç†å»ºè®®"""
        display(HTML("<h3>æ•°æ®é¢„å¤„ç†å»ºè®®</h3>"))
        
        try:
            display(Markdown("""
            åŸºäºå¯¹FDAåŒ»ç–—è®¾å¤‡æ•°æ®çš„åˆ†æï¼Œä»¥ä¸‹æ˜¯é’ˆå¯¹ä¸åŒåˆ†æåœºæ™¯çš„æ•°æ®é¢„å¤„ç†å»ºè®®ï¼š
            
            ### 1. æ—¥æœŸå’Œæ—¶é—´é¢„å¤„ç†
            
            FDAåŒ»ç–—è®¾å¤‡æ•°æ®åº“ä¸­çš„æ—¥æœŸå­—æ®µå¯èƒ½å­˜åœ¨å¤šç§é—®é¢˜ï¼ŒåŒ…æ‹¬ç¼ºå¤±å€¼ã€æœªæ¥æ—¥æœŸå’Œå¼‚å¸¸æ—©æœŸæ—¥æœŸã€‚å»ºè®®çš„é¢„å¤„ç†æ­¥éª¤ï¼š
            
            ```python
            import pandas as pd
            import numpy as np
            from datetime import datetime, timedelta
            
            # åŠ è½½æ•°æ®
            df = pd.read_sql("SELECT * FROM device.adverse_events LIMIT 1000000", conn)
            
            # 1. è½¬æ¢æ—¥æœŸåˆ—
            date_columns = ['date_received', 'date_of_event', 'date_report']
            for col in date_columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
            
            # 2. å¤„ç†æœªæ¥æ—¥æœŸ
            current_date = datetime.now()
            future_threshold = current_date + timedelta(days=30)  # å…è®¸å°èŒƒå›´çš„æœªæ¥æ—¥æœŸï¼ˆä¾‹å¦‚30å¤©ï¼‰
            
            for col in date_columns:
                # å°†è¿‡è¿œçš„æœªæ¥æ—¥æœŸè®¾ç½®ä¸ºNaT
                mask_far_future = df[col] > future_threshold
                df.loc[mask_far_future, col] = pd.NaT
                print(f"å°†{mask_far_future.sum()}æ¡{col}åˆ—çš„è¿œæœŸæœªæ¥æ—¥æœŸè®¾ç½®ä¸ºNaT")
            
            # 3. å¤„ç†å¼‚å¸¸æ—©æœŸæ—¥æœŸ
            min_valid_date = pd.Timestamp('1980-01-01')  # è®¾ç½®åˆç†çš„æœ€æ—©æ—¥æœŸ
            
            for col in date_columns:
                # å°†è¿‡æ—©çš„æ—¥æœŸè®¾ç½®ä¸ºNaT
                mask_too_early = df[col] < min_valid_date
                df.loc[mask_too_early, col] = pd.NaT
                print(f"å°†{mask_too_early.sum()}æ¡{col}åˆ—çš„å¼‚å¸¸æ—©æœŸæ—¥æœŸè®¾ç½®ä¸ºNaT")
            
            # 4. åˆ›å»ºæ—¥æœŸå±‚æ¬¡å­—æ®µ
            for col in date_columns:
                if df[col].notna().any():
                    df[f'{col}_year'] = df[col].dt.year
                    df[f'{col}_month'] = df[col].dt.month
                    df[f'{col}_quarter'] = df[col].dt.quarter
            ```
            
            ### 2. åˆ†ç±»å˜é‡é¢„å¤„ç†
            
            FDAæ•°æ®ä¸­çš„ä»£ç ã€ç±»å‹å’ŒçŠ¶æ€ç­‰åˆ†ç±»å­—æ®µé€šå¸¸å­˜åœ¨æ ‡å‡†åŒ–å’Œä¸€è‡´æ€§é—®é¢˜ï¼š
            
            ```python
            # 1. å­—ç¬¦ä¸²æ ‡å‡†åŒ–
            categorical_columns = ['event_type', 'report_source_code', 'manufacturer_name']
            
            for col in categorical_columns:
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹
                df[col] = df[col].astype(str)
                
                # å»é™¤ç©ºæ ¼å’Œæ ‡å‡†åŒ–å¤§å°å†™
                df[col] = df[col].str.strip().str.upper()
                
                # æ›¿æ¢ç‰¹æ®Šç©ºå€¼è¡¨ç¤º
                df[col] = df[col].replace(['NONE', 'NULL', 'NA', 'NAN', ''], np.nan)
            
            # 2. æ ‡å‡†åŒ–ç‰¹å®šå­—æ®µçš„å€¼
            # ç¤ºä¾‹ï¼šæ ‡å‡†åŒ–ä¸è‰¯äº‹ä»¶ç±»å‹
            event_type_mapping = {
                'INJURY': 'INJURY',
                'INJURIES': 'INJURY',
                'PATIENT INJURY': 'INJURY',
                'MALFUNCTION': 'MALFUNCTION',
                'DEVICE MALFUNCTION': 'MALFUNCTION',
                'DEATH': 'DEATH',
                'PATIENT DEATH': 'DEATH',
                'OTHER': 'OTHER'
            }
            
            df['event_type_std'] = df['event_type'].map(
                lambda x: next((v for k, v in event_type_mapping.items() if k in str(x).upper()), 'OTHER')
            )
            
            # 3. å¤„ç†é«˜åŸºæ•°åˆ†ç±»å˜é‡
            high_cardinality_columns = ['manufacturer_name']
            for col in high_cardinality_columns:
                # æ‰¾å‡ºæœ€å¸¸è§çš„å€¼
                top_values = df[col].value_counts().head(20).index.tolist()
                
                # åˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„åˆ—
                df[f'{col}_grouped'] = df[col].apply(lambda x: x if x in top_values else 'Other')
            ```
            
            ### 3. æ•°å€¼å˜é‡é¢„å¤„ç†
            
            å¤„ç†FDAæ•°æ®ä¸­çš„æ•°å€¼å‹å­—æ®µï¼ŒåŒ…æ‹¬å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†ï¼š
            
            ```python
            import scipy.stats as stats
            
            # 1. è¯†åˆ«æ•°å€¼åˆ—
            numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
            
            # 2. å¼‚å¸¸å€¼å¤„ç†
            for col in numeric_columns:
                if df[col].notna().sum() > 0:
                    # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    # æ£€æµ‹å¼‚å¸¸å€¼
                    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound))
                    print(f"åˆ— {col} ä¸­æ£€æµ‹åˆ° {outliers.sum()} ä¸ªå¼‚å¸¸å€¼")
                    
                    # æ ¹æ®åˆ†æéœ€æ±‚é€‰æ‹©é€‚å½“çš„å¼‚å¸¸å€¼å¤„ç†æ–¹æ³•
                    # é€‰é¡¹1ï¼šæ›¿æ¢ä¸ºè¾¹ç•Œå€¼ï¼ˆå¦‚æœéœ€è¦ä¿ç•™æ•°æ®ç‚¹ï¼‰
                    # df.loc[df[col] < lower_bound, col] = lower_bound
                    # df.loc[df[col] > upper_bound, col] = upper_bound
                    
                    # é€‰é¡¹2ï¼šå°†å¼‚å¸¸å€¼æ›¿æ¢ä¸ºNaNï¼ˆå¦‚æœåˆ†æå…è®¸ç¼ºå¤±å€¼ï¼‰
                    # df.loc[outliers, col] = np.nan
                    
                    # é€‰é¡¹3ï¼šä¸ºå¼‚å¸¸å€¼åˆ›å»ºæ ‡å¿—åˆ—ï¼ˆä¿ç•™åŸå§‹æ•°æ®ä½†æ ‡è®°å¼‚å¸¸å€¼ï¼‰
                    df[f'{col}_is_outlier'] = outliers
            
            # 3. æ ‡å‡†åŒ–/å½’ä¸€åŒ–æ•°å€¼ç‰¹å¾
            from sklearn.preprocessing import StandardScaler, MinMaxScaler
            
            # é€‰æ‹©è¦æ ‡å‡†åŒ–çš„åˆ—
            scale_columns = [col for col in numeric_columns if df[col].notna().sum() > 0]
            
            # æ ‡å‡†åŒ–ï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼‰
            scaler = StandardScaler()
            df[f"{scale_columns}_scaled"] = scaler.fit_transform(df[scale_columns].fillna(0))
            
            # æˆ–è€…å½’ä¸€åŒ–ï¼ˆèŒƒå›´ä»0åˆ°1ï¼‰
            normalizer = MinMaxScaler()
            df[f"{scale_columns}_normalized"] = normalizer.fit_transform(df[scale_columns].fillna(0))
            ```
            
            ### 4. å¤„ç†ç¼ºå¤±å€¼
            
            FDAæ•°æ®ä¸­å­˜åœ¨å¤§é‡ç¼ºå¤±å€¼ï¼Œéœ€è¦æ ¹æ®åˆ†æéœ€æ±‚é‡‡ç”¨é€‚å½“çš„å¤„ç†ç­–ç•¥ï¼š
            
            ```python
            # 1. è®¡ç®—ç¼ºå¤±å€¼æ¯”ä¾‹
            missing_percentages = df.isnull().mean().sort_values(ascending=False) * 100
            print("å„åˆ—ç¼ºå¤±å€¼æ¯”ä¾‹:")
            print(missing_percentages[missing_percentages > 0])
            
            # 2. åˆ é™¤ç¼ºå¤±å€¼æ¯”ä¾‹æé«˜çš„åˆ—ï¼ˆå¯é€‰ï¼‰
            high_missing_cols = missing_percentages[missing_percentages > 90].index.tolist()
            df_reduced = df.drop(columns=high_missing_cols)
            print(f"åˆ é™¤äº† {len(high_missing_cols)} åˆ—ï¼Œç¼ºå¤±å€¼æ¯”ä¾‹è¶…è¿‡90%")
            
            # 3. é’ˆå¯¹ä¸åŒç±»å‹çš„å­—æ®µä½¿ç”¨ä¸åŒçš„å¡«å……ç­–ç•¥
            
            # åˆ†ç±»å­—æ®µå¡«å……ä¸º"Unknown"
            for col in categorical_columns:
                df[col] = df[col].fillna('Unknown')
            
            # æ•°å€¼å­—æ®µå¯ä»¥å¡«å……ä¸ºä¸­ä½æ•°
            for col in numeric_columns:
                df[col] = df[col].fillna(df[col].median())
            
            # æ—¥æœŸå­—æ®µå¯ä»¥ä¿ç•™ä¸ºNaTæˆ–å¡«å……ä¸ºç‰¹å®šå€¼
            # ä¾‹å¦‚ï¼šç”¨æœ€æ—©çš„æœ‰æ•ˆæ—¥æœŸå¡«å……ç¼ºå¤±çš„äº‹ä»¶æ—¥æœŸ
            for col in date_columns:
                df[col + '_missing'] = df[col].isna()  # åˆ›å»ºç¼ºå¤±æŒ‡ç¤ºå™¨
            
            # 4. é«˜çº§å¡«å……æ–¹æ³•ï¼ˆå¯é€‰ï¼‰- ä½¿ç”¨ç›¸å…³åˆ—é¢„æµ‹ç¼ºå¤±å€¼
            from sklearn.impute import KNNImputer
            
            # ç¤ºä¾‹ï¼šä½¿ç”¨KNNä¼°ç®—ç¼ºå¤±å€¼
            # é€‰æ‹©è¦è¿›è¡ŒKNNæ’è¡¥çš„åˆ—
            knn_cols = ['col1', 'col2', 'col3']
            
            # åˆ›å»ºKNNä¼°ç®—å™¨
            imputer = KNNImputer(n_neighbors=5)
            df[knn_cols] = imputer.fit_transform(df[knn_cols])
            ```
            
            ### 5. ç‰¹å¾å·¥ç¨‹
            
            ä¸ºFDAæ•°æ®åˆ›å»ºæœ‰åŠ©äºåˆ†æçš„æ–°ç‰¹å¾ï¼š
            
            ```python
            # 1. æ—¶é—´ç‰¹å¾
            # è®¡ç®—äº‹ä»¶æŠ¥å‘Šå»¶è¿Ÿ
            if 'date_of_event' in df.columns and 'date_received' in df.columns:
                mask = (df['date_of_event'].notna() & df['date_received'].notna())
                df.loc[mask, 'report_delay_days'] = (df.loc[mask, 'date_received'] - df.loc[mask, 'date_of_event']).dt.days
            
            # 2. è®¡ç®—è®¾å¤‡å¹´é¾„ï¼ˆå¦‚æœç›¸å…³å­—æ®µå¯ç”¨ï¼‰
            if 'device_manufacture_date' in df.columns and 'date_of_event' in df.columns:
                mask = (df['device_manufacture_date'].notna() & df['date_of_event'].notna())
                df.loc[mask, 'device_age_days'] = (df.loc[mask, 'date_of_event'] - df.loc[mask, 'device_manufacture_date']).dt.days
            
            # 3. åˆ›å»ºç»„åˆç‰¹å¾
            # ä¾‹å¦‚ï¼šåˆå¹¶è®¾å¤‡ç±»å‹å’Œé—®é¢˜ç±»å‹
            if 'device_class' in df.columns and 'event_type' in df.columns:
                df['device_class_event'] = df['device_class'].astype(str) + '_' + df['event_type'].astype(str)
            
            # 4. åˆ›å»ºåŸºäºæ–‡æœ¬çš„ç‰¹å¾
            if 'reason_for_recall' in df.columns:
                # è®¡ç®—æè¿°é•¿åº¦
                df['reason_length'] = df['reason_for_recall'].str.len()
                
                # åˆ›å»ºå…³é”®è¯æŒ‡ç¤ºå™¨
                keywords = ['battery', 'software', 'contamination', 'sterile', 'label']
                for keyword in keywords:
                    df[f'has_{keyword}'] = df['reason_for_recall'].str.contains(keyword, case=False).astype(int)
            
            # 5. èšåˆç‰¹å¾
            # å¦‚æœæ•°æ®é›†æœ‰å¤šä¸ªè¡¨ï¼Œå¯ä»¥åˆ›å»ºèšåˆç‰¹å¾
            # ä¾‹å¦‚ï¼šè®¡ç®—æ¯ä¸ªè®¾å¤‡ä»£ç ç›¸å…³çš„ä¸è‰¯äº‹ä»¶æ•°é‡
            device_event_counts = df.groupby('product_code')['report_number'].count().reset_index()
            device_event_counts.columns = ['product_code', 'event_count']
            
            # å°†èšåˆç‰¹å¾åˆå¹¶å›ä¸»æ•°æ®æ¡†
            df = df.merge(device_event_counts, on='product_code', how='left')
            ```
            
            ### 6. æ•°æ®é›†åˆ’åˆ†ç­–ç•¥
            
            å¯¹äºæœºå™¨å­¦ä¹ åˆ†æï¼Œéœ€è¦é€‚å½“åˆ’åˆ†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ï¼š
            
            ```python
            from sklearn.model_selection import train_test_split
            
            # 1. åŸºäºæ—¶é—´çš„åˆ’åˆ†ï¼ˆæ¨èç”¨äºæ—¶é—´åºåˆ—æ•°æ®ï¼‰
            if 'date_received' in df.columns:
                # æ ¹æ®æ—¥æœŸæ’åº
                df = df.sort_values('date_received')
                
                # ç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å‰²ç‚¹ï¼ˆä¾‹å¦‚ä½¿ç”¨æœ€å1å¹´çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼‰
                split_date = df['date_received'].max() - pd.Timedelta(days=365)
                
                # åˆ’åˆ†æ•°æ®é›†
                train_df = df[df['date_received'] <= split_date]
                test_df = df[df['date_received'] > split_date]
                
                print(f"è®­ç»ƒé›†: {len(train_df)} è¡Œ ({len(train_df)/len(df):.1%})")
                print(f"æµ‹è¯•é›†: {len(test_df)} è¡Œ ({len(test_df)/len(df):.1%})")
            
            # 2. éšæœºåˆ’åˆ†ï¼ˆé€‚ç”¨äºéæ—¶é—´åºåˆ—æ•°æ®ï¼‰
            else:
                # åˆ’åˆ†ç‰¹å¾å’Œç›®æ ‡å˜é‡
                X = df.drop(columns=['target_column'])
                y = df['target_column']
                
                # åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†
                X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
                X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
                
                print(f"è®­ç»ƒé›†: {len(X_train)} è¡Œ ({len(X_train)/len(X):.1%})")
                print(f"éªŒè¯é›†: {len(X_val)} è¡Œ ({len(X_val)/len(X):.1%})")
                print(f"æµ‹è¯•é›†: {len(X_test)} è¡Œ ({len(X_test)/len(X):.1%})")
            ```
            
            ### 7. æ•°æ®åˆè§„æ€§é¢„å¤„ç†
            
            ç”±äºFDAæ•°æ®å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹éœ€è¦è¿›è¡Œæ•°æ®è„±æ•ï¼š
            
            ```python
            # 1. åˆ é™¤æ•æ„Ÿåˆ—
            sensitive_columns = ['patient_id', 'specific_patient_details']
            df_safe = df.drop(columns=sensitive_columns)
            
            # 2. è„±æ•å¤„ç†
            import hashlib
            
            # å¯¹ç‰¹å®šåˆ—è¿›è¡Œå“ˆå¸Œå¤„ç†
            if 'unique_device_identifier' in df.columns:
                df['hashed_udi'] = df['unique_device_identifier'].apply(
                    lambda x: hashlib.md5(str(x).encode()).hexdigest() if pd.notna(x) else None
                )
                df = df.drop(columns=['unique_device_identifier'])
            
            # 3. åˆ†ç»„/èšåˆæ•æ„Ÿæ•°æ®
            # ä¾‹å¦‚ï¼šå°†ç²¾ç¡®ä½ç½®æ›¿æ¢ä¸ºæ›´å¹¿æ³›çš„åœ°ç†åŒºåŸŸ
            if 'facility_zip' in df.columns:
                df['facility_region'] = df['facility_zip'].str[:3]  # ä½¿ç”¨é‚®ç¼–å‰3ä½è¡¨ç¤ºå¤§è‡´åŒºåŸŸ
                df = df.drop(columns=['facility_zip'])
            ```
            
            ### 8. æ•°æ®è´¨é‡æ£€æŸ¥
            
            åœ¨é¢„å¤„ç†å®Œæˆåï¼Œåº”è¯¥è¿›è¡Œä¸€ç³»åˆ—è´¨é‡æ£€æŸ¥ï¼Œç¡®ä¿æ•°æ®å¯ç”¨äºåˆ†æï¼š
            
            ```python
            # 1. æœ€ç»ˆç¼ºå¤±å€¼æ£€æŸ¥
            final_missing = df.isnull().sum()
            if final_missing.sum() > 0:
                print("è­¦å‘Šï¼šé¢„å¤„ç†åæ•°æ®ä»æœ‰ç¼ºå¤±å€¼:")
                print(final_missing[final_missing > 0])
            
            # 2. æ•°æ®ç±»å‹æ£€æŸ¥
            print("æ•°æ®ç±»å‹æ¦‚è§ˆ:")
            print(df.dtypes)
            
            # 3. æ•°å€¼èŒƒå›´æ£€æŸ¥
            for col in df.select_dtypes(include=['int64', 'float64']).columns:
                print(f"{col}: èŒƒå›´ [{df[col].min()} - {df[col].max()}], å‡å€¼: {df[col].mean():.2f}")
            
            # 4. é‡å¤å€¼æ£€æŸ¥
            duplicates = df.duplicated().sum()
            if duplicates > 0:
                print(f"è­¦å‘Šï¼šæ£€æµ‹åˆ° {duplicates} æ¡é‡å¤è®°å½•")
            
            # 5. ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
            df.to_csv('preprocessed_fda_data.csv', index=False)
            print(f"é¢„å¤„ç†å®Œæˆï¼Œä¿å­˜äº† {len(df)} è¡Œæ•°æ®")
            ```
            
            éµå¾ªè¿™äº›é¢„å¤„ç†æ­¥éª¤å°†å¸®åŠ©æ‚¨å‡†å¤‡FDAåŒ»ç–—è®¾å¤‡æ•°æ®ä»¥è¿›è¡Œå„ç§åˆ†æä»»åŠ¡ï¼ŒåŒ…æ‹¬æè¿°æ€§ç»Ÿè®¡ã€è¶‹åŠ¿åˆ†æã€é¢„æµ‹æ¨¡å‹ç­‰ã€‚æ ¹æ®æ‚¨çš„å…·ä½“åˆ†æç›®æ ‡ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¿™äº›æ­¥éª¤çš„ä¼˜å…ˆçº§æˆ–æ·»åŠ ç‰¹å®šå¤„ç†ã€‚
            """))
            
            # ä¿å­˜é¢„å¤„ç†å»ºè®®
            self.preprocessing_recommendations = {
                'date_handling': [
                    "å¤„ç†æœªæ¥æ—¥æœŸå’Œå¼‚å¸¸æ—©æœŸæ—¥æœŸ",
                    "åˆ›å»ºæ—¥æœŸå±‚æ¬¡å­—æ®µï¼ˆå¹´ã€æœˆã€å­£åº¦ï¼‰",
                    "è®¡ç®—æ—¥æœŸå·®å€¼ï¼ˆå¦‚æŠ¥å‘Šå»¶è¿Ÿï¼‰"
                ],
                'categorical_handling': [
                    "æ ‡å‡†åŒ–åˆ†ç±»å­—æ®µï¼ˆå¤§å°å†™ã€ç©ºæ ¼ã€ç‰¹æ®Šå€¼ï¼‰",
                    "åˆ›å»ºæ ‡å‡†åŒ–æ˜ å°„ä»¥å¤„ç†ä¸ä¸€è‡´çš„å€¼",
                    "å¯¹é«˜åŸºæ•°åˆ†ç±»å˜é‡è¿›è¡Œåˆ†ç»„"
                ],
                'numeric_handling': [
                    "ä½¿ç”¨IQRæˆ–Zåˆ†æ•°æ£€æµ‹å¹¶å¤„ç†å¼‚å¸¸å€¼",
                    "é’ˆå¯¹ç‰¹å®šåˆ†æè¿›è¡Œæ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–",
                    "ä¸ºå¼‚å¸¸å€¼åˆ›å»ºæ ‡å¿—åˆ—"
                ],
                'missing_values': [
                    "æ ¹æ®åˆ†æéœ€æ±‚é‡‡ç”¨ä¸åŒçš„ç¼ºå¤±å€¼ç­–ç•¥",
                    "ä¸ºç¼ºå¤±å€¼åˆ›å»ºæŒ‡ç¤ºå™¨åˆ—",
                    "è€ƒè™‘ä½¿ç”¨KNNæˆ–å…¶ä»–é«˜çº§æ–¹æ³•å¡«å……é‡è¦å˜é‡"
                ],
                'feature_engineering': [
                    "åˆ›å»ºæ—¶é—´ç‰¹å¾ï¼ˆå¦‚å»¶è¿Ÿã€è®¾å¤‡å¹´é¾„ï¼‰",
                    "åˆ›å»ºç»„åˆç‰¹å¾",
                    "ä»æ–‡æœ¬å­—æ®µä¸­æå–å…³é”®è¯ç‰¹å¾",
                    "è®¡ç®—èšåˆç‰¹å¾"
                ]
            }
        
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ•°æ®é¢„å¤„ç†å»ºè®®æ—¶å‡ºé”™: {str(e)}")
    
def generate_analysis_recommendations(self):
    """ç”Ÿæˆåˆ†æå»ºè®®"""
    display(HTML("<h3>åˆ†æå»ºè®®</h3>"))
    
    try:
        display(Markdown("""
        åŸºäºå¯¹FDAåŒ»ç–—è®¾å¤‡æ•°æ®åº“çš„å…¨é¢åˆ†æï¼Œä»¥ä¸‹æ˜¯é’ˆå¯¹ä¸åŒåˆ†æç›®æ ‡çš„å»ºè®®ï¼š
        
        ### 1. æè¿°æ€§ç»Ÿè®¡åˆ†æ
        
        **ç›®æ ‡ï¼š** äº†è§£åŒ»ç–—è®¾å¤‡ä¸è‰¯äº‹ä»¶ã€å¬å›å’Œäº§å“åˆ†å¸ƒçš„åŸºæœ¬æƒ…å†µ
        
        **å»ºè®®åˆ†æï¼š**
        
        - **è®¾å¤‡ç±»å‹åˆ†å¸ƒåˆ†æ**
          - æŒ‰è®¾å¤‡ç±»åˆ«(Class I/II/III)ç»Ÿè®¡äº§å“æ•°é‡å’Œä¸è‰¯äº‹ä»¶æ•°é‡
          - åˆ†æè®¾å¤‡ç±»å‹ä¸é£é™©çº§åˆ«çš„å…³ç³»
        
        - **ä¸è‰¯äº‹ä»¶è¶‹åŠ¿åˆ†æ**
          - æŒ‰å¹´/æœˆ/å­£åº¦ç»Ÿè®¡ä¸è‰¯äº‹ä»¶æ•°é‡
          - åˆ†æä¸åŒäº‹ä»¶ç±»å‹(æ­»äº¡/ä¼¤å®³/æ•…éšœ)çš„æ¯”ä¾‹å˜åŒ–
          - è¯†åˆ«é«˜å³°æœŸå’Œå¯èƒ½çš„å­£èŠ‚æ€§æ¨¡å¼
        
        - **å¬å›æƒ…å†µåˆ†æ**
          - æŒ‰å¬å›åŸå› å’Œåˆ†ç±»ç»Ÿè®¡å¬å›æ•°é‡
          - åˆ†æå¬å›ä¸è®¾å¤‡ç±»å‹çš„å…³ç³»
          - è¯„ä¼°å¬å›å“åº”æ—¶é—´(ä»å‘ç°é—®é¢˜åˆ°å¬å›è¡ŒåŠ¨)
        
        - **åœ°ç†åˆ†å¸ƒåˆ†æ**
          - æŒ‰å›½å®¶/åœ°åŒºåˆ†æä¸è‰¯äº‹ä»¶å’Œå¬å›åˆ†å¸ƒ
          - è¯†åˆ«é«˜é£é™©åœ°åŒºæˆ–æŠ¥å‘Šç‡å¼‚å¸¸çš„åœ°åŒº
        
        **ç¤ºä¾‹SQLæŸ¥è¯¢ï¼š**
        ```sql
        -- æŒ‰è®¾å¤‡ç±»åˆ«ç»Ÿè®¡ä¸è‰¯äº‹ä»¶
        SELECT 
            pc.device_class,
            COUNT(DISTINCT ed.event_id) as event_count,
            COUNT(DISTINCT CASE WHEN ae.event_type = 'Death' THEN ed.event_id END) as death_count,
            COUNT(DISTINCT CASE WHEN ae.event_type = 'Injury' THEN ed.event_id END) as injury_count,
            COUNT(DISTINCT CASE WHEN ae.event_type = 'Malfunction' THEN ed.event_id END) as malfunction_count
        FROM 
            device.product_codes pc
        JOIN 
            device.event_devices ed ON pc.id = ed.product_code_id
        JOIN 
            device.adverse_events ae ON ed.event_id = ae.id
        WHERE 
            pc.device_class IS NOT NULL
        GROUP BY 
            pc.device_class
        ORDER BY 
            event_count DESC;
        ```
        
        ### 2. è¶‹åŠ¿åˆ†æ
        
        **ç›®æ ‡ï¼š** è¯†åˆ«è®¾å¤‡å®‰å…¨æ€§å’Œè´¨é‡çš„é•¿æœŸè¶‹åŠ¿å’Œæ¨¡å¼
        
        **å»ºè®®åˆ†æï¼š**
        
        - **æ—¶é—´åºåˆ—åˆ†æ**
          - ä½¿ç”¨ç§»åŠ¨å¹³å‡å’Œè¶‹åŠ¿åˆ†è§£æŠ€æœ¯åˆ†æé•¿æœŸè¶‹åŠ¿
          - è¯†åˆ«ä¸è‰¯äº‹ä»¶æŠ¥å‘Šä¸­çš„å­£èŠ‚æ€§æ¨¡å¼
          - è¯„ä¼°æ”¿ç­–å˜åŒ–å¯¹æŠ¥å‘Šç‡çš„å½±å“
        
        - **è®¾å¤‡ç”Ÿå‘½å‘¨æœŸåˆ†æ**
          - åˆ†æè®¾å¤‡ä»ä¸Šå¸‚åˆ°å‡ºç°é¦–æ¬¡ä¸è‰¯äº‹ä»¶çš„æ—¶é—´
          - è¯„ä¼°è®¾å¤‡ç±»å‹ä¸é—®é¢˜å‡ºç°æ—¶é—´çš„å…³ç³»
          - è¯†åˆ«äº§å“ç”Ÿå‘½å‘¨æœŸä¸­çš„é«˜é£é™©æœŸ
        
        - **äº§å“ä»£ç è¶‹åŠ¿æ¯”è¾ƒ**
          - æ¯”è¾ƒä¸åŒäº§å“ä»£ç çš„ä¸è‰¯äº‹ä»¶è¶‹åŠ¿
          - è¯†åˆ«é—®é¢˜ç‡ä¸Šå‡æˆ–ä¸‹é™çš„äº§å“ç±»åˆ«
          - è¯„ä¼°ç‰¹å®šäº§å“ä»£ç çš„é£é™©å˜åŒ–
        
        **ç¤ºä¾‹Pythonåˆ†æï¼š**
        ```python
        import pandas as pd
        import matplotlib.pyplot as plt
        from statsmodels.tsa.seasonal import seasonal_decompose
        
        # åŠ è½½æ•°æ®
        query = '''
            SELECT 
                DATE_TRUNC('month', date_received) as month,
                COUNT(*) as event_count
            FROM 
                device.adverse_events
            WHERE 
                date_received BETWEEN '2010-01-01' AND CURRENT_DATE
            GROUP BY 
                month
            ORDER BY 
                month
        '''
        df = pd.read_sql(query, conn)
        df.set_index('month', inplace=True)
        
        # æ‰§è¡Œæ—¶é—´åºåˆ—åˆ†è§£
        result = seasonal_decompose(df['event_count'], model='multiplicative', period=12)
        
        # ç»˜åˆ¶ç»“æœ
        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10))
        result.observed.plot(ax=ax1, title='åŸå§‹ä¸è‰¯äº‹ä»¶æ—¶é—´åºåˆ—')
        result.trend.plot(ax=ax2, title='è¶‹åŠ¿æˆåˆ†')
        result.seasonal.plot(ax=ax3, title='å­£èŠ‚æ€§æˆåˆ†')
        result.resid.plot(ax=ax4, title='æ®‹å·®æˆåˆ†')
        plt.tight_layout()
        plt.show()
        ```
        
        ### 3. å…³è”åˆ†æ
        
        **ç›®æ ‡ï¼š** è¯†åˆ«è®¾å¤‡ç‰¹æ€§ã€é—®é¢˜ç±»å‹å’Œç»“æœä¹‹é—´çš„å…³ç³»
        
        **å»ºè®®åˆ†æï¼š**
        
        - **è®¾å¤‡ç‰¹æ€§ä¸ä¸è‰¯äº‹ä»¶å…³è”**
          - åˆ†æè®¾å¤‡ç±»åˆ«ã€ææ–™ã€ä½¿ç”¨ç¯å¢ƒä¸ä¸è‰¯äº‹ä»¶ç±»å‹çš„å…³ç³»
          - è¯†åˆ«ä¸ç‰¹å®šé—®é¢˜é«˜åº¦ç›¸å…³çš„è®¾å¤‡ç‰¹æ€§
          - è¯„ä¼°è®¾å¤‡å¤æ‚æ€§ä¸æ•…éšœç‡çš„å…³ç³»
        
        - **å…¬å¸è¡¨ç°åˆ†æ**
          - æ¯”è¾ƒä¸åŒå…¬å¸çš„ä¸è‰¯äº‹ä»¶ç‡å’Œå¬å›é¢‘ç‡
          - åˆ†æå…¬å¸è§„æ¨¡ä¸äº§å“è´¨é‡çš„å…³ç³»
          - è¯†åˆ«è´¨é‡è¡¨ç°å¼‚å¸¸çš„åˆ¶é€ å•†
        
        - **é—®é¢˜ç±»å‹ç½‘ç»œåˆ†æ**
          - æ„å»ºé—®é¢˜ä»£ç çš„å…±ç°ç½‘ç»œ
          - è¯†åˆ«ç»å¸¸ä¸€èµ·å‡ºç°çš„é—®é¢˜ç±»å‹
          - å‘ç°æ½œåœ¨çš„å› æœé“¾å’Œé£é™©æ¨¡å¼
        
        **ç¤ºä¾‹åˆ†æä»£ç ï¼š**
        ```python
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # åŠ è½½æ•°æ®
        query = '''
            SELECT 
                c.name as company_name,
                COUNT(DISTINCT ae.id) as adverse_event_count,
                COUNT(DISTINCT CASE WHEN ae.event_type = 'Death' THEN ae.id END) as death_count,
                COUNT(DISTINCT dr.id) as recall_count
            FROM 
                device.companies c
            LEFT JOIN 
                device.adverse_events ae ON c.id = ae.company_id
            LEFT JOIN 
                device.device_recalls dr ON c.id = dr.company_id
            GROUP BY 
                c.name
            HAVING 
                COUNT(DISTINCT ae.id) > 10
            ORDER BY 
                adverse_event_count DESC
            LIMIT 50
        '''
        df = pd.read_sql(query, conn)
        
        # è®¡ç®—æ¯”ç‡
        df['death_ratio'] = df['death_count'] / df['adverse_event_count']
        df['recall_ratio'] = df['recall_count'] / df['adverse_event_count']
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        plt.figure(figsize=(10, 8))
        sns.scatterplot(data=df, x='adverse_event_count', y='recall_ratio', size='death_ratio', 
                       sizes=(20, 200), alpha=0.7, hue='death_ratio')
        
        plt.title('å…¬å¸ä¸è‰¯äº‹ä»¶ã€å¬å›ç‡å’Œæ­»äº¡ç‡å…³ç³»å›¾')
        plt.xlabel('ä¸è‰¯äº‹ä»¶æ•°é‡')
        plt.ylabel('å¬å›ç‡(å¬å›æ•°/ä¸è‰¯äº‹ä»¶æ•°)')
        plt.xscale('log')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        ```
        
        ### 4. é£é™©é¢„æµ‹æ¨¡å‹
        
        **ç›®æ ‡ï¼š** å¼€å‘æ¨¡å‹é¢„æµ‹è®¾å¤‡é£é™©å’Œé—®é¢˜æ¦‚ç‡
        
        **å»ºè®®åˆ†æï¼š**
        
        - **ä¸è‰¯äº‹ä»¶é£é™©è¯„ä¼°æ¨¡å‹**
          - ä½¿ç”¨è®¾å¤‡ç‰¹æ€§å’Œå†å²æ•°æ®é¢„æµ‹ä¸è‰¯äº‹ä»¶é£é™©
          - å¼€å‘é£é™©è¯„åˆ†ç³»ç»Ÿä»¥è¯†åˆ«é«˜é£é™©è®¾å¤‡
          - æ„å»ºè®¾å¤‡ç±»åˆ«çš„é£é™©æ¯”è¾ƒæ¡†æ¶
        
        - **å¬å›é¢„æµ‹æ¨¡å‹**
          - åŸºäºæ—©æœŸä¸è‰¯äº‹ä»¶æ¨¡å¼é¢„æµ‹æœªæ¥å¬å›å¯èƒ½æ€§
          - è¯†åˆ«ä¸é«˜å¬å›ç‡ç›¸å…³çš„è®¾å¤‡ç‰¹æ€§å’Œå…¬å¸ç‰¹å¾
          - å¼€å‘å¬å›é£é™©é¢„è­¦ç³»ç»Ÿ
        
        - **æ–‡æœ¬åˆ†ææ¨¡å‹**
          - ä»ä¸è‰¯äº‹ä»¶æè¿°å’Œå¬å›åŸå› ä¸­æå–å…³é”®ä¿¡æ¯
          - ä½¿ç”¨NLPæŠ€æœ¯å¯¹é£é™©æè¿°è¿›è¡Œåˆ†ç±»
          - å¼€å‘äº‹ä»¶ä¸¥é‡æ€§è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿ
        
        **ç¤ºä¾‹æ¨¡å‹å¼€å‘ä»£ç ï¼š**
        ```python
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import classification_report, roc_auc_score
        from sklearn.preprocessing import StandardScaler
        
        # å‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº†é¢„å¤„ç†å¥½çš„æ•°æ®é›†
        # ç‰¹å¾åŒ…æ‹¬è®¾å¤‡ç±»å‹ã€å…¬å¸å†å²ã€è®¾å¤‡ææ–™ç­‰
        # ç›®æ ‡å˜é‡æ˜¯è®¾å¤‡æ˜¯å¦å‘ç”Ÿä¸¥é‡ä¸è‰¯äº‹ä»¶
        
        # å‡†å¤‡æ•°æ®
        X = df[['device_class', 'company_event_history', 'is_implant', 'is_reusable', 
               'company_age', 'previous_recalls', 'device_complexity']]
        y = df['had_serious_event']
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        y_pred = model.predict(X_test_scaled)
        y_prob = model.predict_proba(X_test_scaled)[:, 1]
        
        print(classification_report(y_test, y_pred))
        print(f"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}")
        
        # åˆ†æç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("ç‰¹å¾é‡è¦æ€§:")
        print(feature_importance)
        ```
        
        ### 5. ç½‘ç»œå’Œå›¾åˆ†æ
        
        **ç›®æ ‡ï¼š** æ¢ç´¢è®¾å¤‡ã€å…¬å¸å’Œé—®é¢˜ä¹‹é—´çš„å…³ç³»ç½‘ç»œ
        
        **å»ºè®®åˆ†æï¼š**
        
        - **å…¬å¸-äº§å“ç½‘ç»œ**
          - æ„å»ºå…¬å¸ä¸äº§å“ä¹‹é—´çš„äºŒéƒ¨å›¾
          - è¯†åˆ«å…±äº«æŠ€æœ¯æˆ–é—®é¢˜çš„å…¬å¸é›†ç¾¤
          - åˆ†æé—®é¢˜ä¼ æ’­æ¨¡å¼
        
        - **äº§å“é—®é¢˜å…³è”ç½‘ç»œ**
          - åˆ›å»ºåŸºäºå…±åŒé—®é¢˜çš„äº§å“ç›¸ä¼¼æ€§ç½‘ç»œ
          - è¯†åˆ«äº§å“ç±»åˆ«ä¸­çš„é£é™©ç°‡
          - è¯„ä¼°é—®é¢˜çš„ä¼ é€’æ€§å’Œç›¸å…³æ€§
        
        - **ç›‘ç®¡è¡ŒåŠ¨å½±å“å›¾**
          - åˆ†æç›‘ç®¡è¡ŒåŠ¨å¯¹ä¸è‰¯äº‹ä»¶æŠ¥å‘Šçš„å½±å“
          - è¯„ä¼°ç›‘ç®¡ç½‘ç»œçš„è¦†ç›–èŒƒå›´å’Œæ•ˆæœ
          - è¯†åˆ«ç›‘ç®¡ç›²ç‚¹æˆ–é‡ç‚¹å…³æ³¨åŒºåŸŸ
        
        **ç¤ºä¾‹ç½‘ç»œåˆ†æä»£ç ï¼š**
        ```python
        import pandas as pd
        import numpy as np
        import networkx as nx
        import matplotlib.pyplot as plt
        
        # æ„å»ºå…¬å¸-äº§å“ç½‘ç»œ
        query = '''
            SELECT 
                c.name as company_name,
                pc.product_code,
                COUNT(DISTINCT ae.id) as connection_strength
            FROM 
                device.companies c
            JOIN 
                device.adverse_events ae ON c.id = ae.company_id
            JOIN 
                device.event_devices ed ON ae.id = ed.event_id
            JOIN 
                device.product_codes pc ON ed.product_code_id = pc.id
            GROUP BY 
                c.name, pc.product_code
            HAVING 
                COUNT(DISTINCT ae.id) > 5
        '''
        edges_df = pd.read_sql(query, conn)
        
        # åˆ›å»ºäºŒéƒ¨å›¾
        G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for company in edges_df['company_name'].unique():
            G.add_node(company, node_type='company')
            
        for product in edges_df['product_code'].unique():
            G.add_node(product, node_type='product')
        
        # æ·»åŠ è¾¹
        for _, row in edges_df.iterrows():
            G.add_edge(row['company_name'], row['product_code'], weight=row['connection_strength'])
        
        # è®¡ç®—ç½‘ç»œæŒ‡æ ‡
        company_centrality = nx.degree_centrality(G)
        top_companies = sorted([(node, cent) for node, cent in company_centrality.items() 
                               if G.nodes[node].get('node_type') == 'company'],
                              key=lambda x: x[1], reverse=True)[:10]
        
        print("ç½‘ç»œä¸­æœ€ä¸­å¿ƒçš„å…¬å¸:")
        for company, cent in top_companies:
            print(f"{company}: {cent:.4f}")
            
        # å¯è§†åŒ–ç½‘ç»œ
        plt.figure(figsize=(12, 12))
        pos = nx.spring_layout(G, k=0.3)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        company_nodes = [node for node, attr in G.nodes(data=True) if attr.get('node_type') == 'company']
        product_nodes = [node for node, attr in G.nodes(data=True) if attr.get('node_type') == 'product']
        
        nx.draw_networkx_nodes(G, pos, nodelist=company_nodes, node_color='red', node_size=100, alpha=0.8)
        nx.draw_networkx_nodes(G, pos, nodelist=product_nodes, node_color='blue', node_size=50, alpha=0.8)
        
        # ç»˜åˆ¶è¾¹
        edges = G.edges(data=True)
        weights = [data['weight'] for _, _, data in edges]
        nx.draw_networkx_edges(G, pos, width=[w/10 for w in weights], alpha=0.3)
        
        plt.title('å…¬å¸-äº§å“å…³è”ç½‘ç»œ')
        plt.axis('off')
        plt.tight_layout()
        plt.show()
        ```
        
        ### 6. æ–‡æœ¬æŒ–æ˜åˆ†æ
        
        **ç›®æ ‡ï¼š** ä»ä¸è‰¯äº‹ä»¶æè¿°å’Œå¬å›æ–‡æœ¬ä¸­æå–è§è§£
        
        **å»ºè®®åˆ†æï¼š**
        
        - **å…³é”®è¯æå–å’Œè¶‹åŠ¿**
          - ä»ä¸è‰¯äº‹ä»¶æŠ¥å‘Šä¸­æå–å¸¸è§é—®é¢˜æè¿°
          - è·Ÿè¸ªå…³é”®æœ¯è¯­ä½¿ç”¨é¢‘ç‡çš„å˜åŒ–
          - è¯†åˆ«æ–°å‡ºç°çš„é£é™©æœ¯è¯­
        
        - **ä¸»é¢˜å»ºæ¨¡**
          - ä½¿ç”¨LDAæˆ–å…¶ä»–ä¸»é¢˜æ¨¡å‹åˆ†æäº‹ä»¶æè¿°
          - è¯†åˆ«å¸¸è§é—®é¢˜ç±»å‹å’Œä¸»é¢˜
          - æ¢ç´¢ä¸»é¢˜éšæ—¶é—´çš„æ¼”å˜
        
        - **æƒ…æ„Ÿåˆ†æ**
          - åˆ†æä¸è‰¯äº‹ä»¶æŠ¥å‘Šä¸­çš„æœ¯è¯­ä¸¥é‡æ€§
          - è¯„ä¼°æŠ¥å‘Šè¯­è¨€ä¸äº‹ä»¶ä¸¥é‡ç¨‹åº¦çš„å…³ç³»
          - å‘ç°æŠ¥å‘Šåå·®å’Œæ¨¡å¼
        
        **ç¤ºä¾‹æ–‡æœ¬åˆ†æä»£ç ï¼š**
        ```python
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import re
        from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
        from sklearn.decomposition import LatentDirichletAllocation
        from wordcloud import WordCloud
        
        # åŠ è½½æ–‡æœ¬æ•°æ®
        query = '''
            SELECT 
                et.text,
                ae.event_type,
                ae.date_received
            FROM 
                device.event_texts et
            JOIN 
                device.adverse_events ae ON et.event_id = ae.id
            WHERE 
                et.text_type_code = 'Description'
                AND ae.date_received > '2015-01-01'
            LIMIT 10000
        '''
        df = pd.read_sql(query, conn)
        
        # æ–‡æœ¬é¢„å¤„ç†
        def preprocess_text(text):
            if pd.isna(text):
                return ""
            # è½¬æ¢ä¸ºå°å†™
            text = text.lower()
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            text = re.sub(r'[^\\w\\s]', '', text)
            # ç§»é™¤æ•°å­—
            text = re.sub(r'\\d+', '', text)
            # ç§»é™¤å¤šä½™ç©ºæ ¼
            text = re.sub(r'\\s+', ' ', text).strip()
            return text
        
        df['processed_text'] = df['text'].apply(preprocess_text)
        
        # åˆ›å»ºTF-IDFå‘é‡
        tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', min_df=5)
        tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])
        
        # ä¸»é¢˜å»ºæ¨¡
        lda = LatentDirichletAllocation(n_components=5, random_state=42)
        lda.fit(tfidf_matrix)
        
        # æ˜¾ç¤ºä¸»é¢˜
        feature_names = tfidf_vectorizer.get_feature_names_out()
        
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[:-11:-1]
            top_words = [feature_names[i] for i in top_words_idx]
            print(f"ä¸»é¢˜ #{topic_idx+1}:")
            print(", ".join(top_words))
            print()
        
        # æ ¹æ®äº‹ä»¶ç±»å‹ç”Ÿæˆè¯äº‘
        event_types = df['event_type'].unique()
        
        fig, axes = plt.subplots(1, len(event_types), figsize=(15, 5))
        
        for i, event_type in enumerate(event_types):
            text = " ".join(df[df['event_type'] == event_type]['processed_text'])
            
            wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)
            
            axes[i].imshow(wordcloud, interpolation='bilinear')
            axes[i].set_title(f'äº‹ä»¶ç±»å‹: {event_type}')
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.show()
        ```
        
        ### 7. åœ°ç†ç©ºé—´åˆ†æ
        
        **ç›®æ ‡ï¼š** æ¢ç´¢è®¾å¤‡é—®é¢˜çš„åœ°ç†åˆ†å¸ƒå’Œæ¨¡å¼
        
        **å»ºè®®åˆ†æï¼š**
        
        - **åœ°ç†çƒ­ç‚¹åˆ†æ**
          - ç»˜åˆ¶ä¸è‰¯äº‹ä»¶å’Œå¬å›çš„åœ°ç†åˆ†å¸ƒ
          - è¯†åˆ«é—®é¢˜é«˜å‘åœ°åŒº
          - åˆ†æåŒºåŸŸæŠ¥å‘Šç‡å·®å¼‚
        
        - **ç©ºé—´èšç±»åˆ†æ**
          - ä½¿ç”¨ç©ºé—´èšç±»ç®—æ³•è¯†åˆ«ç›¸ä¼¼é—®é¢˜åŒºåŸŸ
          - åˆ†æåŒºåŸŸä¸è®¾å¤‡ç±»å‹çš„å…³ç³»
          - è¯„ä¼°åœ°ç†ä½ç½®ä¸ç»“æœä¸¥é‡æ€§çš„å…³ç³»
        
        - **åŒºåŸŸæ¯”è¾ƒåˆ†æ**
          - æ¯”è¾ƒä¸åŒåŒºåŸŸçš„è®¾å¤‡é—®é¢˜æ¨¡å¼
          - åˆ†æåŒºåŸŸç›‘ç®¡å·®å¼‚çš„å½±å“
          - è¯†åˆ«è·¨åŒºåŸŸé£é™©ä¼ æ’­æ¨¡å¼
        
        **ç¤ºä¾‹åœ°ç†åˆ†æï¼š**
        ```python
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import geopandas as gpd
        
        # åŠ è½½åœ°ç†æ•°æ®
        usa = gpd.read_file('usa_shapefile.shp')
        
        # åŠ è½½å¸¦åœ°ç†ä¿¡æ¯çš„æ•°æ®
        query = '''
            SELECT 
                reporter_state as state,
                COUNT(*) as event_count
            FROM 
                device.adverse_events
            WHERE 
                reporter_state IS NOT NULL
                AND reporter_state != ''
            GROUP BY 
                reporter_state
        '''
        state_counts = pd.read_sql(query, conn)
        
        # åˆå¹¶åœ°ç†æ•°æ®å’Œäº‹ä»¶æ•°æ®
        merged = usa.merge(state_counts, left_on='STATE_ABBR', right_on='state', how='left')
        merged['event_count'] = merged['event_count'].fillna(0)
        
        # ç»˜åˆ¶åœ°å›¾
        fig, ax = plt.subplots(1, 1, figsize=(15, 10))
        merged.plot(column='event_count', ax=ax, legend=True, cmap='OrRd', 
                   legend_kwds={'label': "ä¸è‰¯äº‹ä»¶æ•°é‡", 'orientation': "horizontal"})
        
        plt.title('ç¾å›½å„å·åŒ»ç–—è®¾å¤‡ä¸è‰¯äº‹ä»¶åˆ†å¸ƒ')
        plt.axis('off')
        plt.tight_layout()
        plt.show()
        
        # è®¡ç®—äººå£è°ƒæ•´åçš„ç‡
        population_data = pd.read_csv('state_population.csv')
        merged = merged.merge(population_data, left_on='STATE_ABBR', right_on='state_abbr', how='left')
        
        # è®¡ç®—æ¯åä¸‡äººå£çš„äº‹ä»¶ç‡
        merged['event_rate'] = (merged['event_count'] / merged['population']) * 100000
        
        # ç»˜åˆ¶äººå£è°ƒæ•´åçš„åœ°å›¾
        fig, ax = plt.subplots(1, 1, figsize=(15, 10))
        merged.plot(column='event_rate', ax=ax, legend=True, cmap='OrRd', 
                   legend_kwds={'label': "æ¯åä¸‡äººå£ä¸è‰¯äº‹ä»¶æ•°", 'orientation': "horizontal"})
        
        plt.title('ç¾å›½å„å·åŒ»ç–—è®¾å¤‡ä¸è‰¯äº‹ä»¶ç‡(äººå£è°ƒæ•´å)')
        plt.axis('off')
        plt.tight_layout()
        plt.show()
        ```
        
        ### 8. ç›‘ç®¡æœ‰æ•ˆæ€§åˆ†æ
        
        **ç›®æ ‡ï¼š** è¯„ä¼°ç›‘ç®¡è¡ŒåŠ¨å¯¹è®¾å¤‡å®‰å…¨æ€§çš„å½±å“
        
        **å»ºè®®åˆ†æï¼š**
        
        - **æ”¿ç­–å½±å“è¯„ä¼°**
          - åˆ†æç›‘ç®¡å˜åŒ–å‰åçš„ä¸è‰¯äº‹ä»¶å’Œå¬å›æ¨¡å¼
          - ä½¿ç”¨ä¸­æ–­æ—¶é—´åºåˆ—æ–¹æ³•è¯„ä¼°æ”¿ç­–æ•ˆæœ
          - è¯†åˆ«æœ‰æ•ˆå’Œä½æ•ˆçš„ç›‘ç®¡å¹²é¢„
        
        - **é£é™©å“åº”åˆ†æ**
          - åˆ†æä»é—®é¢˜è¯†åˆ«åˆ°ç›‘ç®¡å“åº”çš„æ—¶é—´
          - è¯„ä¼°ç›‘ç®¡æªæ–½ä¸å‡å°‘äº‹ä»¶çš„å…³ç³»
          - æ¯”è¾ƒä¸åŒç±»å‹ç›‘ç®¡è¡ŒåŠ¨çš„æ•ˆæœ
        
        - **åˆè§„è¡¨ç°åˆ†æ**
          - åˆ†æå…¬å¸è¿‡å»åˆè§„å†å²ä¸ä¸è‰¯äº‹ä»¶çš„å…³ç³»
          - è¯„ä¼°ç›‘ç®¡è­¦å‘Šåçš„å…¬å¸è¡¨ç°å˜åŒ–
          - è¯†åˆ«éœ€è¦åŠ å¼ºç›‘ç®¡çš„é¢†åŸŸ
        
        **ç¤ºä¾‹åˆ†æä»£ç ï¼š**
        ```python
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import statsmodels.api as sm
        from statsmodels.tsa.statespace.sarimax import SARIMAX
        
        # åŠ è½½ç›‘ç®¡å˜åŒ–å‰åçš„äº‹ä»¶æ•°æ®
        # å‡è®¾æˆ‘ä»¬åœ¨2018å¹´1æœˆæœ‰ä¸€é¡¹é‡è¦çš„ç›‘ç®¡å˜åŒ–
        intervention_date = '2018-01-01'
        
        query = '''
            SELECT 
                DATE_TRUNC('month', date_received) as month,
                COUNT(*) as event_count
            FROM 
                device.adverse_events
            WHERE 
                date_received BETWEEN '2015-01-01' AND '2021-12-31'
            GROUP BY 
                month
            ORDER BY 
                month
        '''
        df = pd.read_sql(query, conn)
        df['month'] = pd.to_datetime(df['month'])
        df.set_index('month', inplace=True)
        
        # åˆ›å»ºå¹²é¢„æ ‡å¿—
        df['intervention'] = df.index >= intervention_date
        
        # ä¸­æ–­æ—¶é—´åºåˆ—åˆ†æ
        model = SARIMAX(df['event_count'], 
                       exog=df['intervention'],
                       order=(1, 1, 1),
                       seasonal_order=(1, 1, 1, 12))
        
        results = model.fit()
        print(results.summary())
        
        # ç»˜åˆ¶ç»“æœ
        plt.figure(figsize=(12, 6))
        plt.plot(df.index, df['event_count'], label='å®é™…äº‹ä»¶æ•°')
        
        # é¢„æµ‹ç»“æœ
        pred = results.get_prediction()
        plt.plot(df.index, pred.predicted_mean, label='æ¨¡å‹é¢„æµ‹', color='red')
        
        # ç»˜åˆ¶å¹²é¢„çº¿
        intervention_idx = df.index.get_loc(pd.to_datetime(intervention_date))
        plt.axvline(x=pd.to_datetime(intervention_date), color='green', linestyle='--')
        plt.text(pd.to_datetime(intervention_date), df['event_count'].max() * 0.9, 
                'ç›‘ç®¡å˜åŒ–', rotation=90)
        
        plt.title('ç›‘ç®¡å˜åŒ–å¯¹ä¸è‰¯äº‹ä»¶æŠ¥å‘Šçš„å½±å“')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('æœˆåº¦ä¸è‰¯äº‹ä»¶æ•°')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        ```
        """))
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆåˆ†æå»ºè®®æ—¶å‡ºé”™: {str(e)}")

# ç¤ºä¾‹ä½¿ç”¨æ–¹æ³•
if __name__ == "__main__":
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®åº“è¿æ¥ä¿¡æ¯
    from config import DB_CONFIG
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = DataQualityAnalyzer(DB_CONFIG)
    
    # è¿æ¥æ•°æ®åº“
    if analyzer.connect():
        # è¿è¡Œå®Œæ•´åˆ†æ
        analyzer.run_full_analysis()
        
        # å…³é—­è¿æ¥
        analyzer.close()
    else:
        print("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥é…ç½®ä¿¡æ¯ã€‚")